{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhiljalan/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_init=([[ 0.0901572 ,  0.2370918 ,  0.6920835 , -0.45759007, -0.22167274,\n",
    "        -0.46439773, -0.45912468,  0.6203555 ,  0.0419175 ,  0.60444146,\n",
    "         4.952244  ,  0.04192306,  0.53345317,  0.22216071, -0.24009007,\n",
    "         6.301405  , -0.50758445, -0.21116066, -0.37131187, -0.22089699,\n",
    "         0.04239784,  0.04331616, -0.18591626, -0.22142634, -0.4953288 ,\n",
    "        -0.23889707,  0.67850924,  0.5476355 , -0.7077681 , -1.0123378 ,\n",
    "         0.04195131,  0.22627208, -0.1888109 ,  0.21195143,  0.44928712,\n",
    "         0.04276987, -0.20532611,  0.44252077, -0.04190878,  0.46343717,\n",
    "        -0.22356562, -0.5474644 ,  0.04206235, -0.7823536 , -0.23852947,\n",
    "         0.26123488, -0.2369954 , -0.25654712, -0.25827566,  0.5539032 ,\n",
    "         0.22289808,  0.51685596,  1.0848937 , -0.6088887 , -0.04201594,\n",
    "         0.21767725, -0.23810348, -0.4646694 , -0.53889185,  1.1317953 ,\n",
    "         0.2089353 , -0.23368704, -5.6309223 , -0.2510263 ,  0.71514434,\n",
    "         1.2417319 ,  5.88868   ,  0.4928691 ,  0.2434442 , -0.54655886,\n",
    "         0.6717308 ,  0.44354093, -0.7333635 , -0.6745134 , -0.04279398,\n",
    "        -0.7975697 ,  0.22850451, -0.25397167,  0.2451518 ,  1.1024855 ,\n",
    "        -0.53172445,  0.04208738, -0.04233624,  0.8983515 ,  0.7710562 ,\n",
    "        -0.2548618 , -0.21645324, -1.0170518 ,  0.9672949 , -0.23664552,\n",
    "        -0.22946735,  0.63287175, -0.79163665, -0.52115196,  0.21819146,\n",
    "        -0.22541553,  0.69617873,  0.73459744,  0.50693244, -0.24401082,\n",
    "        -0.5940728 ,  1.3320855 , -1.140783  ,  0.23237722, -1.1244652 ,\n",
    "        -5.6705046 ,  0.2540727 , -0.04189253, -0.20804366, -0.04187457,\n",
    "        -0.21428825,  0.04335834,  0.96757776, -5.0284066 , -0.21626869,\n",
    "        -0.540456  ,  0.51839244,  0.21898666,  0.9066629 ,  0.22020821,\n",
    "        -0.50667083,  0.7983404 , -5.5656185 , -0.04212693,  0.25555643,\n",
    "        -0.45822552,  0.24277431, -0.04205061,  0.15989499,  0.23738208,\n",
    "         0.2237451 ,  0.24180941,  0.49051645, -0.45438182,  0.47147265,\n",
    "        -0.04477705, -5.479455  ,  0.04174316,  0.2551995 ,  0.57939404,\n",
    "        -0.6557258 , -0.04206115,  0.6763663 ,  0.23443314,  0.22873235,\n",
    "        -0.04198467, -0.4861976 , -0.6498148 ,  0.44098404, -0.04172933]])\n",
    "w2_init=([[-0.84504426],\n",
    "       [-0.51247114],\n",
    "       [-2.0340562 ],\n",
    "       [-0.76634175],\n",
    "       [ 0.61729795],\n",
    "       [-0.58101785],\n",
    "       [-0.6854419 ],\n",
    "       [ 0.6577067 ],\n",
    "       [-0.7736458 ],\n",
    "       [-1.8916265 ],\n",
    "       [-1.090016  ],\n",
    "       [-0.873359  ],\n",
    "       [ 0.42003942],\n",
    "       [-0.47995704],\n",
    "       [ 0.5497382 ],\n",
    "       [-2.1801522 ],\n",
    "       [-0.4831816 ],\n",
    "       [ 0.5648663 ],\n",
    "       [ 0.9415591 ],\n",
    "       [ 0.78689337],\n",
    "       [-0.91083336],\n",
    "       [-0.9763873 ],\n",
    "       [ 0.72957134],\n",
    "       [ 0.5560705 ],\n",
    "       [-0.4719117 ],\n",
    "       [ 0.5045661 ],\n",
    "       [ 0.66004866],\n",
    "       [-1.5987552 ],\n",
    "       [-0.4643787 ],\n",
    "       [-1.9016262 ],\n",
    "       [-0.96371204],\n",
    "       [-0.611284  ],\n",
    "       [ 0.65741754],\n",
    "       [-0.5599199 ],\n",
    "       [ 0.45351097],\n",
    "       [-0.97737604],\n",
    "       [ 0.7038435 ],\n",
    "       [ 0.5943796 ],\n",
    "       [ 0.9532466 ],\n",
    "       [ 0.7460163 ],\n",
    "       [ 0.5358916 ],\n",
    "       [-0.44170648],\n",
    "       [-0.9419488 ],\n",
    "       [-0.67798716],\n",
    "       [ 0.46497133],\n",
    "       [-0.391163  ],\n",
    "       [ 0.592325  ],\n",
    "       [ 0.45341557],\n",
    "       [ 0.43128943],\n",
    "       [ 0.41603804],\n",
    "       [-0.5674596 ],\n",
    "       [ 0.38761157],\n",
    "       [ 2.704492  ],\n",
    "       [-0.80798954],\n",
    "       [ 0.83548236],\n",
    "       [-0.5111326 ],\n",
    "       [ 0.6162054 ],\n",
    "       [-0.7550416 ],\n",
    "       [-0.4759281 ],\n",
    "       [-2.5150294 ],\n",
    "       [-0.50941396],\n",
    "       [ 0.49656197],\n",
    "       [-1.6215047 ],\n",
    "       [ 0.47244617],\n",
    "       [ 0.5376818 ],\n",
    "       [ 3.9775271 ],\n",
    "       [ 1.6411495 ],\n",
    "       [ 0.45862758],\n",
    "       [-0.47453666],\n",
    "       [-0.45376387],\n",
    "       [ 0.5765134 ],\n",
    "       [ 0.56581146],\n",
    "       [-1.1258857 ],\n",
    "       [-1.0639522 ],\n",
    "       [ 1.0760058 ],\n",
    "       [-1.235642  ],\n",
    "       [-0.53190786],\n",
    "       [ 0.47500044],\n",
    "       [-0.4640562 ],\n",
    "       [ 2.372436  ],\n",
    "       [-0.67921394],\n",
    "       [-1.0515941 ],\n",
    "       [ 1.1015248 ],\n",
    "       [ 1.4750271 ],\n",
    "       [-2.5024996 ],\n",
    "       [ 0.43387246],\n",
    "       [ 0.53801376],\n",
    "       [-2.327031  ],\n",
    "       [ 1.6461738 ],\n",
    "       [ 0.4792684 ],\n",
    "       [ 0.76675403],\n",
    "       [ 0.4892529 ],\n",
    "       [-1.1853842 ],\n",
    "       [-0.38456675],\n",
    "       [-0.80742   ],\n",
    "       [ 0.45512152],\n",
    "       [ 0.44872195],\n",
    "       [-2.1801472 ],\n",
    "       [ 0.67657053],\n",
    "       [ 0.40404373],\n",
    "       [-0.7937116 ],\n",
    "       [ 0.77783364],\n",
    "       [-2.4614215 ],\n",
    "       [-0.6792038 ],\n",
    "       [ 2.5339882 ],\n",
    "       [-1.5957985 ],\n",
    "       [-0.4930483 ],\n",
    "       [ 0.9237745 ],\n",
    "       [ 0.59356   ],\n",
    "       [ 0.9956936 ],\n",
    "       [ 0.47309944],\n",
    "       [-0.9341501 ],\n",
    "       [ 1.6710144 ],\n",
    "       [ 1.1764897 ],\n",
    "       [ 0.46367607],\n",
    "       [-0.7061653 ],\n",
    "       [ 0.46270266],\n",
    "       [-0.8225886 ],\n",
    "       [ 1.8290645 ],\n",
    "       [-0.5919749 ],\n",
    "       [-0.44208294],\n",
    "       [-1.948723  ],\n",
    "       [-1.3858926 ],\n",
    "       [ 0.8691517 ],\n",
    "       [-0.37294617],\n",
    "       [-0.6558015 ],\n",
    "       [-0.6871818 ],\n",
    "       [ 1.0781469 ],\n",
    "       [-0.87414324],\n",
    "       [-0.47635847],\n",
    "       [-0.5639866 ],\n",
    "       [-0.47552544],\n",
    "       [ 0.7286468 ],\n",
    "       [-0.34246516],\n",
    "       [ 0.6627983 ],\n",
    "       [ 0.7922385 ],\n",
    "       [-0.80032754],\n",
    "       [-0.6089186 ],\n",
    "       [-0.46824703],\n",
    "       [ 0.40888965],\n",
    "       [-0.56078476],\n",
    "       [ 0.98349524],\n",
    "       [ 0.48105317],\n",
    "       [-0.5328922 ],\n",
    "       [-0.70839876],\n",
    "       [ 1.0339078 ],\n",
    "       [-0.61342776],\n",
    "       [-0.79129976],\n",
    "       [ 0.48441455],\n",
    "       [ 0.5570059 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_init=([-1.49632066e-01,  2.16088206e-01,  3.65778732e+00, -1.21041000e+00,\n",
    "       -1.35061651e-01, -1.29561055e+00, -1.22450840e+00, -2.32706118e+00,\n",
    "       -2.15838999e-02,  3.23842049e+00,  9.99821246e-01,  5.85471094e-02,\n",
    "        1.77022383e-01,  1.33129925e-01, -2.35600263e-01, -9.69530642e-01,\n",
    "        7.31552601e-01, -9.77801457e-02, -1.28652573e+00,  2.19140470e-01,\n",
    "        1.23102725e-01, -1.57810926e-01,  1.53959572e-01, -1.32225156e-01,\n",
    "       -1.57481730e-01, -2.27377295e-01,  4.70594555e-01,  2.85312033e+00,\n",
    "        3.12517256e-01,  5.74599028e+00, -2.24734023e-02,  1.56200081e-01,\n",
    "       -7.49236792e-02,  9.45027769e-02, -9.54202712e-01, -1.19746946e-01,\n",
    "        1.76245585e-01, -1.47855604e+00,  1.07089831e-02,  1.27336562e+00,\n",
    "        2.21104607e-01, -1.81072652e-01,  8.26996788e-02, -5.77640235e-01,\n",
    "       -2.25629151e-01,  3.77086610e-01, -2.16601476e-01, -3.45170379e-01,\n",
    "       -3.56887221e-01, -5.90745807e-01, -2.19919547e-01, -1.86245930e+00,\n",
    "        6.39037895e+00,  2.27631497e+00, -7.94772431e-02,  1.15748756e-01,\n",
    "        3.03080708e-01, -1.28756618e+00, -1.78790972e-01, -5.63836622e+00,\n",
    "        8.99272710e-02, -1.94928318e-01, -7.41466433e-02, -3.07720184e-01,\n",
    "        2.70801663e-01,  7.34310913e+00,  8.29299539e-02, -7.81100869e-01,\n",
    "        2.56538272e-01, -1.80862710e-01,  2.18636543e-01,  1.07038522e+00,\n",
    "       -2.78851628e+00, -2.51557636e+00,  1.20577067e-01, -3.08366776e+00,\n",
    "        1.66032478e-01, -3.27756613e-01,  2.67747581e-01, -6.31493044e+00,\n",
    "       -1.79744363e+00, -4.68141548e-02,  7.84308538e-02, -5.00692749e+00,\n",
    "        4.00230837e+00, -3.33558679e-01, -1.12384461e-01, -5.97595739e+00,\n",
    "       -5.45763254e+00, -2.12760210e-01,  2.53413409e-01,  1.98413730e-01,\n",
    "        4.21520996e+00,  6.86769903e-01, -2.12254256e-01, -1.46499500e-01,\n",
    "        4.68130678e-01, -4.72452021e+00, -1.81595242e+00, -2.60216951e-01,\n",
    "        2.21049786e+00, -1.94112194e+00,  6.55437994e+00, -2.68400759e-01,\n",
    "        5.60166454e+00, -7.55500719e-02,  3.28553319e-01,  6.42770529e-03,\n",
    "       -9.20422822e-02, -1.11987339e-02, -1.00595385e-01, -1.61407873e-01,\n",
    "       -5.45945311e+00, -1.01744628e+00, -1.06990181e-01,  1.96982336e+00,\n",
    "        1.70830369e-01, -2.16641054e-01,  5.29849386e+00,  1.28267542e-01,\n",
    "        7.34108150e-01, -4.16245031e+00, -7.15808198e-02, -9.45318416e-02,\n",
    "        3.37766856e-01, -1.21507788e+00, -3.34076196e-01,  4.01906781e-02,\n",
    "       -1.60489708e-01,  2.17334837e-01,  1.42836973e-01,  2.45796412e-01,\n",
    "        1.53452313e+00,  9.28530157e-01,  1.37115467e+00,  1.37233928e-01,\n",
    "       -6.79805875e-02,  4.52714004e-02,  3.36023450e-01, -2.64137276e-02,\n",
    "       -2.08564326e-01,  4.37483490e-02,  2.18686923e-01,  2.00063869e-01,\n",
    "       -2.48323262e-01,  2.81832628e-02, -1.50140417e+00,  2.45667958e+00,\n",
    "       -9.98386204e-01, -4.18332741e-02])\n",
    "b2_init= ([-0.47102088])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_init_data = {}\n",
    "# encoder_init_data['w1'] = np.asarray(w1_init)\n",
    "# encoder_init_data['w2'] = np.asarray(w2_init)\n",
    "# encoder_init_data['b1'] = np.asarray(b1_init)\n",
    "# encoder_init_data['b2'] = np.asarray(b2_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shelve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with shelve.open('intermediate_values') as db:\n",
    "#     db['encoder_stepfn_tanh_id_weights'] = encoder_init_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_weights(encoder_vars, decoder_vars): \n",
    "    #     encoder_vars, decoder_vars = entry[0], entry[1]\n",
    "    for i in range(len(encoder_vars)): \n",
    "        encoder_vars[i] = np.ndarray.flatten(encoder_vars[i])\n",
    "        decoder_vars[i] = np.ndarray.flatten(decoder_vars[i])\n",
    "    W1, b1, W2, b2 = encoder_vars[0], encoder_vars[1], encoder_vars[2], encoder_vars[3]\n",
    "    W3, b3, W4, b4 = decoder_vars[0], decoder_vars[1], decoder_vars[2], decoder_vars[3]\n",
    "    return np.hstack((W1, b1, W2, b2, W3, b3, W4, b4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_norm_update(weight_stack_1, weight_stack_2): \n",
    "    weight_diff = weight_stack_1 - weight_stack_2\n",
    "    l2_norm = LA.norm(weight_diff) / LA.norm(weight_stack_1)\n",
    "    l1_norm = np.sum(np.abs(weight_diff)) / np.sum(np.abs(weight_stack_1))\n",
    "    return l1_norm, l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_squared = 0.04\n",
    "x0 = tf.placeholder(tf.float32, [None, 1])\n",
    "z = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(initial_value=w1_init, name='W1')\n",
    "b1 = tf.Variable(b1_init, name='b1')\n",
    "# and the weights connecting the hidden layer to the u1 output layer\n",
    "W2 = tf.Variable(w2_init, name='W2')\n",
    "b2 = tf.Variable(b2_init, name='b2')\n",
    "\n",
    "#how to specify shape without weight initialization?\n",
    "# declare weights connecting x1+z to a hidden layer\n",
    "xavier_init = tf.glorot_uniform_initializer()\n",
    "\n",
    "W3 = tf.Variable(xavier_init([1, 150]), name='W3') #todo dense layer init\n",
    "b3 = tf.Variable(initial_value=tf.zeros([150]), name='b3')\n",
    "# and the weights connecting the hidden layer to the u1 output layer\n",
    "W4 = tf.Variable(xavier_init([150, 1]), expected_shape=[150, 1], name='W4')\n",
    "b4 = tf.Variable(initial_value=tf.zeros([150]), name='b4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "\n",
    "# calculate the output of the hidden layer\n",
    "hidden_out_1 = tf.add(tf.matmul(x0, W1), b1)\n",
    "hidden_out_1 = tf.nn.tanh(hidden_out_1)\n",
    "\n",
    "# # output layer\n",
    "u1 = tf.identity(tf.add(tf.matmul(hidden_out_1, W2), b2))\n",
    "# print(u1.get_shape())\n",
    "# x1 = u1 + x0\n",
    "x1 = u1 + x0\n",
    "\n",
    "#Decoder\n",
    "# add noise to x1\n",
    "\n",
    "# x1_noise = x1\n",
    "x1_noise = x1 + z\n",
    "\n",
    "hidden_out_2 = tf.nn.sigmoid(tf.add(tf.matmul(x1_noise, W3), b3))\n",
    "u2 = tf.identity(tf.add(tf.matmul(hidden_out_2, W4), b4))\n",
    "\n",
    "x2 = x1 - u2\n",
    "\n",
    "u1_cost = k_squared * tf.reduce_mean(tf.reduce_sum((u1)**2, axis=1))\n",
    "x2_cost = tf.reduce_mean(tf.reduce_sum((x2)**2, axis=1))\n",
    "wits_cost = x2_cost #+u1_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal: set a much lower learning rate for the encoder than the decoder, given that we've learned \n",
    "#a step function from the encoder. \n",
    "encoder_vars = [W1, b1, W2, b2]\n",
    "decoder_vars = [W3, b3, W4, b4]\n",
    "encoder_opt = tf.train.GradientDescentOptimizer(learning_rate = 0) #adam? \n",
    "decoder_opt = tf.train.GradientDescentOptimizer(learning_rate = 5e-5)\n",
    "\n",
    "grads = tf.gradients(wits_cost, encoder_vars + decoder_vars)\n",
    "grads1 = grads[:len(encoder_vars)]\n",
    "grads2 = grads[len(encoder_vars):]\n",
    "\n",
    "train_op1 = encoder_opt.apply_gradients(zip(grads1, encoder_vars))\n",
    "train_op2 = decoder_opt.apply_gradients(zip(grads2, decoder_vars))\n",
    "train_op = tf.group(train_op1, train_op2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stddev = 5\n",
    "# x_train = np.random.normal(size=num_epochs * batch_size, scale=x_stddev)\n",
    "x_test = np.linspace(-3 * x_stddev, 3 * x_stddev, num=300)\n",
    "z_test = np.random.normal(size=(300,), scale = 1.0)\n",
    "epoch_step = int(num_epochs/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM INITIALIZATION NUMBER: 1\n",
      "Step 0, Cost 5153.5166015625, MC Cost: 4009.027099609375\n",
      "Step 200, Cost 172.6238250732422, MC Cost: 171.6248779296875\n",
      "Step 400, Cost 156.2258758544922, MC Cost: 160.26927185058594\n",
      "Step 600, Cost 146.21661376953125, MC Cost: 156.01014709472656\n",
      "Step 800, Cost 152.23690795898438, MC Cost: 153.36766052246094\n",
      "RANDOM INITIALIZATION NUMBER: 2\n",
      "Step 0, Cost 4698.70654296875, MC Cost: 3586.279541015625\n",
      "Step 200, Cost 175.1435089111328, MC Cost: 171.48365783691406\n",
      "Step 400, Cost 158.89662170410156, MC Cost: 160.78265380859375\n",
      "Step 600, Cost 164.60678100585938, MC Cost: 157.07662963867188\n",
      "Step 800, Cost 153.39418029785156, MC Cost: 153.43441772460938\n",
      "RANDOM INITIALIZATION NUMBER: 3\n",
      "Step 0, Cost 4249.509765625, MC Cost: 3579.974365234375\n",
      "Step 200, Cost 175.38644409179688, MC Cost: 173.45758056640625\n",
      "Step 400, Cost 155.5950164794922, MC Cost: 160.77479553222656\n",
      "Step 600, Cost 162.29518127441406, MC Cost: 156.617431640625\n",
      "Step 800, Cost 154.7482452392578, MC Cost: 153.71099853515625\n",
      "RANDOM INITIALIZATION NUMBER: 4\n",
      "Step 0, Cost 4517.099609375, MC Cost: 4199.982421875\n",
      "Step 200, Cost 173.9745635986328, MC Cost: 173.0458526611328\n",
      "Step 400, Cost 173.74266052246094, MC Cost: 160.4907989501953\n",
      "Step 600, Cost 155.52247619628906, MC Cost: 156.0301055908203\n",
      "Step 800, Cost 147.9530487060547, MC Cost: 153.5936737060547\n",
      "RANDOM INITIALIZATION NUMBER: 5\n",
      "Step 0, Cost 4290.82373046875, MC Cost: 3848.7412109375\n",
      "Step 200, Cost 165.1273193359375, MC Cost: 172.02586364746094\n",
      "Step 400, Cost 172.03616333007812, MC Cost: 160.0926971435547\n",
      "Step 600, Cost 148.3659210205078, MC Cost: 155.79583740234375\n",
      "Step 800, Cost 132.90113830566406, MC Cost: 153.3955841064453\n",
      "RANDOM INITIALIZATION NUMBER: 6\n",
      "Step 0, Cost 3423.1484375, MC Cost: 3271.679931640625\n",
      "Step 200, Cost 163.78207397460938, MC Cost: 170.97959899902344\n",
      "Step 400, Cost 156.21444702148438, MC Cost: 159.92147827148438\n",
      "Step 600, Cost 147.6638946533203, MC Cost: 155.74346923828125\n",
      "Step 800, Cost 148.92239379882812, MC Cost: 153.467529296875\n",
      "RANDOM INITIALIZATION NUMBER: 7\n",
      "Step 0, Cost 4241.662109375, MC Cost: 3868.8232421875\n",
      "Step 200, Cost 151.8511199951172, MC Cost: 171.8080596923828\n",
      "Step 400, Cost 153.38937377929688, MC Cost: 160.03805541992188\n",
      "Step 600, Cost 167.45692443847656, MC Cost: 155.55859375\n",
      "Step 800, Cost 154.9775848388672, MC Cost: 153.9655303955078\n",
      "RANDOM INITIALIZATION NUMBER: 8\n",
      "Step 0, Cost 4050.925537109375, MC Cost: 3939.1953125\n",
      "Step 200, Cost 169.39051818847656, MC Cost: 172.88514709472656\n",
      "Step 400, Cost 163.57516479492188, MC Cost: 160.71435546875\n",
      "Step 600, Cost 157.85296630859375, MC Cost: 156.031494140625\n",
      "Step 800, Cost 157.29966735839844, MC Cost: 153.79783630371094\n",
      "RANDOM INITIALIZATION NUMBER: 9\n",
      "Step 0, Cost 4876.2041015625, MC Cost: 3510.521240234375\n",
      "Step 200, Cost 194.77076721191406, MC Cost: 170.9043731689453\n",
      "Step 400, Cost 142.03309631347656, MC Cost: 160.02386474609375\n",
      "Step 600, Cost 164.0938720703125, MC Cost: 156.04420471191406\n",
      "Step 800, Cost 159.79946899414062, MC Cost: 153.4825439453125\n",
      "RANDOM INITIALIZATION NUMBER: 10\n",
      "Step 0, Cost 4511.0908203125, MC Cost: 3869.23681640625\n",
      "Step 200, Cost 164.73362731933594, MC Cost: 173.42759704589844\n",
      "Step 400, Cost 166.0311279296875, MC Cost: 160.88671875\n",
      "Step 600, Cost 155.21473693847656, MC Cost: 156.0416717529297\n",
      "Step 800, Cost 142.09698486328125, MC Cost: 153.57823181152344\n",
      "RANDOM INITIALIZATION NUMBER: 11\n",
      "Step 0, Cost 4108.69580078125, MC Cost: 3393.22998046875\n",
      "Step 200, Cost 182.92576599121094, MC Cost: 172.62718200683594\n",
      "Step 400, Cost 182.43997192382812, MC Cost: 160.6002655029297\n",
      "Step 600, Cost 159.43145751953125, MC Cost: 155.99844360351562\n",
      "Step 800, Cost 167.35520935058594, MC Cost: 153.60421752929688\n",
      "RANDOM INITIALIZATION NUMBER: 12\n",
      "Step 0, Cost 4650.166015625, MC Cost: 3849.44189453125\n",
      "Step 200, Cost 168.7732391357422, MC Cost: 171.93954467773438\n",
      "Step 400, Cost 157.60116577148438, MC Cost: 160.1197509765625\n",
      "Step 600, Cost 161.5509033203125, MC Cost: 155.66403198242188\n",
      "Step 800, Cost 163.11256408691406, MC Cost: 153.37356567382812\n",
      "RANDOM INITIALIZATION NUMBER: 13\n",
      "Step 0, Cost 4950.962890625, MC Cost: 3756.583984375\n",
      "Step 200, Cost 171.67831420898438, MC Cost: 171.531005859375\n",
      "Step 400, Cost 164.33164978027344, MC Cost: 160.20924377441406\n",
      "Step 600, Cost 154.7563018798828, MC Cost: 155.66395568847656\n",
      "Step 800, Cost 149.31570434570312, MC Cost: 153.40628051757812\n",
      "RANDOM INITIALIZATION NUMBER: 14\n",
      "Step 0, Cost 4431.2578125, MC Cost: 3422.642333984375\n",
      "Step 200, Cost 180.51223754882812, MC Cost: 172.32891845703125\n",
      "Step 400, Cost 171.14151000976562, MC Cost: 160.6016082763672\n",
      "Step 600, Cost 146.15391540527344, MC Cost: 155.7686767578125\n",
      "Step 800, Cost 158.03854370117188, MC Cost: 153.5438690185547\n",
      "RANDOM INITIALIZATION NUMBER: 15\n",
      "Step 0, Cost 4398.50634765625, MC Cost: 3742.8798828125\n",
      "Step 200, Cost 167.23667907714844, MC Cost: 171.3229522705078\n",
      "Step 400, Cost 165.34661865234375, MC Cost: 159.77133178710938\n",
      "Step 600, Cost 160.8315887451172, MC Cost: 155.6737060546875\n",
      "Step 800, Cost 152.07235717773438, MC Cost: 153.111572265625\n",
      "RANDOM INITIALIZATION NUMBER: 16\n",
      "Step 0, Cost 4516.0859375, MC Cost: 3858.662353515625\n",
      "Step 200, Cost 177.27352905273438, MC Cost: 171.0004119873047\n",
      "Step 400, Cost 150.475830078125, MC Cost: 159.66600036621094\n",
      "Step 600, Cost 151.16607666015625, MC Cost: 155.42904663085938\n",
      "Step 800, Cost 146.13516235351562, MC Cost: 153.42092895507812\n",
      "RANDOM INITIALIZATION NUMBER: 17\n",
      "Step 0, Cost 4958.90966796875, MC Cost: 3881.038818359375\n",
      "Step 200, Cost 162.89688110351562, MC Cost: 172.37539672851562\n",
      "Step 400, Cost 164.8490447998047, MC Cost: 160.36880493164062\n",
      "Step 600, Cost 165.38038635253906, MC Cost: 156.04212951660156\n",
      "Step 800, Cost 155.2997283935547, MC Cost: 153.36984252929688\n",
      "RANDOM INITIALIZATION NUMBER: 18\n",
      "Step 0, Cost 4759.34814453125, MC Cost: 3711.14794921875\n",
      "Step 200, Cost 162.33026123046875, MC Cost: 171.7757568359375\n",
      "Step 400, Cost 157.05996704101562, MC Cost: 160.14132690429688\n",
      "Step 600, Cost 157.9526824951172, MC Cost: 155.72137451171875\n",
      "Step 800, Cost 161.8314971923828, MC Cost: 153.38555908203125\n",
      "RANDOM INITIALIZATION NUMBER: 19\n",
      "Step 0, Cost 5001.2333984375, MC Cost: 3922.496826171875\n",
      "Step 200, Cost 175.83030700683594, MC Cost: 173.1697540283203\n",
      "Step 400, Cost 163.68246459960938, MC Cost: 160.4544677734375\n",
      "Step 600, Cost 157.85360717773438, MC Cost: 155.99240112304688\n",
      "Step 800, Cost 159.04269409179688, MC Cost: 153.8429412841797\n",
      "RANDOM INITIALIZATION NUMBER: 20\n",
      "Step 0, Cost 4652.9091796875, MC Cost: 3722.6298828125\n",
      "Step 200, Cost 189.87637329101562, MC Cost: 172.77442932128906\n",
      "Step 400, Cost 152.27513122558594, MC Cost: 160.18734741210938\n",
      "Step 600, Cost 148.5779571533203, MC Cost: 156.28025817871094\n",
      "Step 800, Cost 142.5383758544922, MC Cost: 153.68739318847656\n",
      "RANDOM INITIALIZATION NUMBER: 21\n",
      "Step 0, Cost 4158.00927734375, MC Cost: 3680.197509765625\n",
      "Step 200, Cost 173.17831420898438, MC Cost: 172.89923095703125\n",
      "Step 400, Cost 175.38595581054688, MC Cost: 161.04794311523438\n",
      "Step 600, Cost 147.6961212158203, MC Cost: 155.8539276123047\n",
      "Step 800, Cost 163.44346618652344, MC Cost: 153.45352172851562\n",
      "RANDOM INITIALIZATION NUMBER: 22\n",
      "Step 0, Cost 4963.72216796875, MC Cost: 4101.3447265625\n",
      "Step 200, Cost 179.0601806640625, MC Cost: 170.63137817382812\n",
      "Step 400, Cost 151.575927734375, MC Cost: 159.9037322998047\n",
      "Step 600, Cost 159.31863403320312, MC Cost: 155.67095947265625\n",
      "Step 800, Cost 160.82196044921875, MC Cost: 153.1552276611328\n",
      "RANDOM INITIALIZATION NUMBER: 23\n",
      "Step 0, Cost 4523.07666015625, MC Cost: 3577.306396484375\n",
      "Step 200, Cost 179.69873046875, MC Cost: 170.9725799560547\n",
      "Step 400, Cost 164.31585693359375, MC Cost: 159.7044219970703\n",
      "Step 600, Cost 161.6715087890625, MC Cost: 155.57627868652344\n",
      "Step 800, Cost 138.7887725830078, MC Cost: 153.38070678710938\n",
      "RANDOM INITIALIZATION NUMBER: 24\n",
      "Step 0, Cost 4852.28271484375, MC Cost: 4171.521484375\n",
      "Step 200, Cost 187.94219970703125, MC Cost: 172.62606811523438\n",
      "Step 400, Cost 161.5275421142578, MC Cost: 160.8502655029297\n",
      "Step 600, Cost 150.43856811523438, MC Cost: 156.43154907226562\n",
      "Step 800, Cost 156.40838623046875, MC Cost: 153.94271850585938\n",
      "RANDOM INITIALIZATION NUMBER: 25\n",
      "Step 0, Cost 3833.011962890625, MC Cost: 3462.348876953125\n",
      "Step 200, Cost 164.75347900390625, MC Cost: 171.1020050048828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400, Cost 148.43450927734375, MC Cost: 159.84207153320312\n",
      "Step 600, Cost 154.2308349609375, MC Cost: 155.5930938720703\n",
      "Step 800, Cost 147.75306701660156, MC Cost: 153.89263916015625\n",
      "RANDOM INITIALIZATION NUMBER: 26\n",
      "Step 0, Cost 4608.4140625, MC Cost: 3867.318359375\n",
      "Step 200, Cost 155.9092254638672, MC Cost: 171.32044982910156\n",
      "Step 400, Cost 164.69898986816406, MC Cost: 159.9412078857422\n",
      "Step 600, Cost 166.06387329101562, MC Cost: 155.93734741210938\n",
      "Step 800, Cost 149.59658813476562, MC Cost: 153.92474365234375\n",
      "RANDOM INITIALIZATION NUMBER: 27\n",
      "Step 0, Cost 4807.12109375, MC Cost: 3812.55126953125\n",
      "Step 200, Cost 187.06248474121094, MC Cost: 171.68663024902344\n",
      "Step 400, Cost 167.42454528808594, MC Cost: 160.28134155273438\n",
      "Step 600, Cost 166.39306640625, MC Cost: 155.75975036621094\n",
      "Step 800, Cost 151.44778442382812, MC Cost: 153.38021850585938\n",
      "RANDOM INITIALIZATION NUMBER: 28\n",
      "Step 0, Cost 4975.55859375, MC Cost: 4208.3134765625\n",
      "Step 200, Cost 188.10195922851562, MC Cost: 171.9273223876953\n",
      "Step 400, Cost 160.6732635498047, MC Cost: 159.99632263183594\n",
      "Step 600, Cost 163.1221160888672, MC Cost: 155.8967742919922\n",
      "Step 800, Cost 154.91812133789062, MC Cost: 153.34938049316406\n",
      "RANDOM INITIALIZATION NUMBER: 29\n",
      "Step 0, Cost 4929.38916015625, MC Cost: 4138.3291015625\n",
      "Step 200, Cost 170.4145965576172, MC Cost: 171.2596893310547\n",
      "Step 400, Cost 171.49632263183594, MC Cost: 159.7034454345703\n",
      "Step 600, Cost 157.32763671875, MC Cost: 155.57427978515625\n",
      "Step 800, Cost 149.4766387939453, MC Cost: 153.18760681152344\n",
      "RANDOM INITIALIZATION NUMBER: 30\n",
      "Step 0, Cost 4153.34423828125, MC Cost: 3617.555908203125\n",
      "Step 200, Cost 181.684814453125, MC Cost: 172.49659729003906\n",
      "Step 400, Cost 141.2310028076172, MC Cost: 159.9828643798828\n",
      "Step 600, Cost 153.2841033935547, MC Cost: 155.78192138671875\n",
      "Step 800, Cost 167.68360900878906, MC Cost: 153.7012481689453\n",
      "RANDOM INITIALIZATION NUMBER: 31\n",
      "Step 0, Cost 4468.85498046875, MC Cost: 3560.813720703125\n",
      "Step 200, Cost 168.14187622070312, MC Cost: 172.8922576904297\n",
      "Step 400, Cost 161.11680603027344, MC Cost: 160.4207763671875\n",
      "Step 600, Cost 158.5095672607422, MC Cost: 155.97109985351562\n",
      "Step 800, Cost 151.23318481445312, MC Cost: 153.68138122558594\n",
      "RANDOM INITIALIZATION NUMBER: 32\n",
      "Step 0, Cost 4470.20703125, MC Cost: 3634.318359375\n",
      "Step 200, Cost 165.4682159423828, MC Cost: 170.74237060546875\n",
      "Step 400, Cost 149.2980194091797, MC Cost: 159.69956970214844\n",
      "Step 600, Cost 152.8004913330078, MC Cost: 155.7601318359375\n",
      "Step 800, Cost 149.90220642089844, MC Cost: 153.74659729003906\n",
      "RANDOM INITIALIZATION NUMBER: 33\n",
      "Step 0, Cost 5067.88134765625, MC Cost: 4332.6435546875\n",
      "Step 200, Cost 181.4020538330078, MC Cost: 169.9490203857422\n",
      "Step 400, Cost 156.39797973632812, MC Cost: 159.71347045898438\n",
      "Step 600, Cost 144.34835815429688, MC Cost: 155.71072387695312\n",
      "Step 800, Cost 145.7013397216797, MC Cost: 153.59951782226562\n",
      "RANDOM INITIALIZATION NUMBER: 34\n",
      "Step 0, Cost 4237.46630859375, MC Cost: 3714.0888671875\n",
      "Step 200, Cost 172.36935424804688, MC Cost: 170.88568115234375\n",
      "Step 400, Cost 167.77369689941406, MC Cost: 159.99407958984375\n",
      "Step 600, Cost 155.823974609375, MC Cost: 156.14466857910156\n",
      "Step 800, Cost 152.5254669189453, MC Cost: 153.46603393554688\n",
      "RANDOM INITIALIZATION NUMBER: 35\n",
      "Step 0, Cost 4916.423828125, MC Cost: 3862.02490234375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-293cbf40690d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mz_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwits_cost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#             train_cost.append(cost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# all_u1 = []\n",
    "# all_x2 = []\n",
    "# all_u2 = []\n",
    "# all_y2 = []\n",
    "# l1_weight_updates = []\n",
    "# l2_weight_updates = []\n",
    "\n",
    "# train_cost = []\n",
    "# weights_dict = {}\n",
    "# prev_weights = np.zeros(shape=(1051,))\n",
    "\n",
    "mc_x_batch = np.random.normal(size=(5000, 1), scale = x_stddev)\n",
    "mc_z_batch = np.random.normal(size=(5000, 1), scale = 1.0)\n",
    "# mc_losses = []\n",
    "num_random_starts = 100\n",
    "for i in range(num_random_starts):\n",
    "    print('RANDOM INITIALIZATION NUMBER: {}'.format(i + 1))\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #training\n",
    "        for epoch in range(num_epochs): \n",
    "            x_batch = np.random.normal(size=(batch_size, 1), scale = x_stddev)\n",
    "            z_batch = np.random.normal(size=(batch_size, 1), scale = 1.0)\n",
    "\n",
    "            _, cost,  = sess.run([train_op, wits_cost], feed_dict={x0: x_batch, z: z_batch})\n",
    "\n",
    "#             train_cost.append(cost)\n",
    "\n",
    "            mc_cost = sess.run([wits_cost], feed_dict={x0: mc_x_batch, z: mc_z_batch})\n",
    "#             mc_losses.append(mc_cost[0])\n",
    "\n",
    "#             next_weights = stack_weights(encoder_vars_tmp, decoder_vars_tmp)\n",
    "#             l1_update, l2_update = weight_norm_update(prev_weights, next_weights)\n",
    "#             l1_weight_updates.append(l1_update)\n",
    "#             l2_weight_updates.append(l2_update)\n",
    "#             prev_weights = next_weights\n",
    "\n",
    "            if epoch % epoch_step == 0: \n",
    "                print('Step {}, Cost {}, MC Cost: {}'.format(epoch, cost, mc_cost[0]))\n",
    "\n",
    "#         for i in range(len(x_test)): \n",
    "#             u1_temp, y2_temp, u2_temp, x2_temp = sess.run([u1, x1_noise, u2, x2], \n",
    "#                                                              feed_dict={x0: x_test[i].reshape(1,1), \n",
    "#                                                                         z: z_test[i].reshape(1, 1)})\n",
    "#             all_u1.append(u1_temp[0][0])\n",
    "#             all_x2.append(x2_temp[0][0])\n",
    "#             all_y2.append(y2_temp[0][0])\n",
    "#             all_u2.append(u2_temp[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12.0, 8.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fractional_losses = [1]\n",
    "# for i in range(1, len(train_cost)): \n",
    "#     fractional_losses.append((train_cost[i] - train_cost[i-1])/train_cost[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_first = epoch_step * 2\n",
    "epoch_num_range = np.arange(0, num_epochs)[ignore_first:]\n",
    "plt.plot(epoch_num_range, mc_losses[ignore_first:], c='blue', label='MC Loss')\n",
    "plt.plot(epoch_num_range, train_cost[ignore_first:], c='red', linestyle='--', label='Train Loss', alpha=0.3)\n",
    "plt.xlabel('Epoch', fontsize=28)\n",
    "plt.ylabel('Train Loss', fontsize=28)\n",
    "plt.title('Training history', fontsize=28)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_first = int(num_epochs/10)\n",
    "epoch_num_range = np.arange(0, num_epochs)[ignore_first:]\n",
    "plt.plot(epoch_num_range, mc_losses[ignore_first:], c='blue', label='MC Loss')\n",
    "plt.plot(epoch_num_range, train_cost[ignore_first:], c='red', linestyle='--', label='Train Loss', alpha=0.3)\n",
    "plt.xlabel('Epoch', fontsize=28)\n",
    "plt.ylabel('Train Loss', fontsize=28)\n",
    "plt.title('Training history', fontsize=28)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(epoch_num_range, mc_losses[ignore_first:], c='orange', label='Fractional Loss', s=2.0)\n",
    "plt.plot(epoch_num_range, l1_weight_updates[ignore_first:], c='blue', label='Fractional l1 Weight Update')\n",
    "plt.plot(epoch_num_range, l2_weight_updates[ignore_first:], c='red', alpha=0.3, linestyle='--', label='Fractional l2 Weight Update')\n",
    "plt.xlabel('Epoch', fontsize=28)\n",
    "plt.semilogy()\n",
    "plt.legend();\n",
    "plt.title('Fractional Update in Loss vs in Weights (log-scaled)', fontsize=20);\n",
    "# plt.savefig('figs/fixed_encoder_test/with_noise_train_weight_history_logscale.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_num_range, mc_losses[ignore_first:], c='blue', label='Fractional Loss')\n",
    "plt.plot(epoch_num_range, l1_weight_updates[ignore_first:], c='orange', label='Fractional l1 Weight Update')\n",
    "plt.plot(epoch_num_range, l2_weight_updates[ignore_first:], c='green', label='Fractional l2 Weight Update')\n",
    "plt.xlabel('Epoch', fontsize=28)\n",
    "# plt.semilogy()\n",
    "plt.legend();\n",
    "plt.title('Fractional Update in Loss vs in Weights (no scaling)', fontsize=20)\n",
    "# plt.savefig('figs/fixed_encoder_test/with_noise_train_weight_history.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test, all_x2, s=2.5)\n",
    "plt.title('No y2 noise, fixed encoder, sigmoid-identity decoder')\n",
    "plt.xlabel('x0', fontsize=28)\n",
    "plt.ylabel('x2', fontsize=28)\n",
    "# plt.savefig('figs/fixed_encoder_test/with_noise_x0_vs_x2.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test, all_u1, s=2.5)\n",
    "plt.xlabel('x0', fontsize=28)\n",
    "plt.ylabel('u1', fontsize=28);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test, all_u1 + x_test, s=2.5)\n",
    "plt.xlabel('x0', fontsize=28)\n",
    "plt.ylabel('x1', fontsize=28);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all_y2, all_u2, s=2.5)\n",
    "plt.title('No y2 noise, fixed encoder, sigmoid-identity decoder')\n",
    "plt.xlabel('y2', fontsize=28)\n",
    "plt.ylabel('u2', fontsize=28)\n",
    "# plt.savefig('figs/fixed_encoder_test/with_noise_x0_vs_x2.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
