NOTE: Decay was always 1 - 1e-10
RUN NUMBER 1
RUNNING FOR learning_rate: 0.01, num_units_1: 100, num_units_2: 20, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 7.79448127746582
step: 100, loss: 6.1842474937438965
step: 200, loss: 6.184765338897705
step: 300, loss: 6.860335350036621
step: 400, loss: 6.127499580383301
step: 500, loss: 6.415597915649414
step: 600, loss: 5.076105117797852
step: 700, loss: 6.158297061920166
step: 800, loss: 4.827503204345703
step: 900, loss: 4.187593936920166
step: 1000, loss: 6.408882141113281
step: 1100, loss: 4.965085506439209
step: 1200, loss: 4.839460372924805
step: 1300, loss: 5.662483215332031
step: 1400, loss: 7.736819267272949
step: 1500, loss: 6.148301124572754
step: 1600, loss: 7.523427963256836
step: 1700, loss: 6.248902797698975
step: 1800, loss: 4.444979190826416
step: 1900, loss: 5.389739990234375
step: 2000, loss: 4.670001983642578
step: 2100, loss: 4.749138832092285
step: 2200, loss: 6.204615592956543
step: 2300, loss: 6.2999396324157715
step: 2400, loss: 5.274940490722656
step: 2500, loss: 5.386729717254639
step: 2600, loss: 4.413128852844238
step: 2700, loss: 5.17031192779541
step: 2800, loss: 4.935579776763916
step: 2900, loss: 4.047085762023926
step: 3000, loss: 5.697844505310059
step: 3100, loss: 5.00153112411499
step: 3200, loss: 6.692032337188721
step: 3300, loss: 5.731916904449463
step: 3400, loss: 5.841122627258301
step: 3500, loss: 5.768633842468262
step: 3600, loss: 4.6034770011901855
step: 3700, loss: 4.313641548156738
step: 3800, loss: 5.883998870849609
step: 3900, loss: 6.562093257904053
step: 4000, loss: 4.945230007171631
step: 4100, loss: 6.701430320739746
step: 4200, loss: 6.369346618652344
step: 4300, loss: 7.148144245147705
step: 4400, loss: 6.172856330871582
step: 4500, loss: 6.286666393280029
step: 4600, loss: 4.782203674316406
step: 4700, loss: 4.397495269775391
step: 4800, loss: 4.235931873321533
step: 4900, loss: 6.837366580963135
step: 5000, loss: 4.493420124053955
step: 5100, loss: 6.1910834312438965
step: 5200, loss: 5.465101718902588
step: 5300, loss: 4.836374282836914
step: 5400, loss: 6.552887916564941
step: 5500, loss: 5.718321323394775
step: 5600, loss: 4.88779878616333
step: 5700, loss: 4.4141154289245605
step: 5800, loss: 5.195400238037109
step: 5900, loss: 6.727102279663086
step: 6000, loss: 5.177371025085449
step: 6100, loss: 5.343442916870117
step: 6200, loss: 4.553933620452881
step: 6300, loss: 6.530763149261475
step: 6400, loss: 5.71290397644043
step: 6500, loss: 5.5225372314453125
step: 6600, loss: 5.136590957641602
step: 6700, loss: 4.915816307067871
step: 6800, loss: 4.777590751647949
step: 6900, loss: 5.5800395011901855
step: 7000, loss: 5.312933444976807
step: 7100, loss: 5.4067702293396
step: 7200, loss: 7.7454304695129395
step: 7300, loss: 7.009335994720459
step: 7400, loss: 4.8654866218566895
step: 7500, loss: 5.027535438537598
step: 7600, loss: 5.201481342315674
step: 7700, loss: 5.389379501342773
step: 7800, loss: 5.0087361335754395
step: 7900, loss: 4.1705641746521
step: 8000, loss: 5.921464920043945
step: 8100, loss: 6.54902458190918
step: 8200, loss: 5.802585124969482
step: 8300, loss: 5.486130714416504
step: 8400, loss: 5.510595798492432
step: 8500, loss: 6.604796409606934
step: 8600, loss: 7.496045112609863
step: 8700, loss: 5.9118332862854
step: 8800, loss: 5.167705059051514
step: 8900, loss: 5.773279666900635
step: 9000, loss: 5.119013786315918
step: 9100, loss: 5.056290149688721
step: 9200, loss: 4.831712245941162
step: 9300, loss: 5.16566276550293
step: 9400, loss: 3.315063238143921
step: 9500, loss: 5.3480048179626465
step: 9600, loss: 4.906703472137451
step: 9700, loss: 4.925863265991211
step: 9800, loss: 4.8524394035339355
step: 9900, loss: 5.267454624176025
step: 10000, loss: 3.6664018630981445
step: 10100, loss: 5.803815841674805
step: 10200, loss: 4.653039932250977
step: 10300, loss: 5.5934739112854
step: 10400, loss: 6.192507743835449
step: 10500, loss: 5.7511305809021
step: 10600, loss: 5.937902927398682
step: 10700, loss: 6.028987884521484
step: 10800, loss: 4.961926460266113
step: 10900, loss: 4.858177185058594
step: 11000, loss: 6.313448905944824
step: 11100, loss: 6.581033229827881
step: 11200, loss: 4.359345436096191
step: 11300, loss: 5.273624897003174
step: 11400, loss: 4.4486403465271
step: 11500, loss: 4.432344436645508
step: 11600, loss: 6.642438888549805
step: 11700, loss: 6.627501964569092
step: 11800, loss: 5.762977123260498
step: 11900, loss: 5.747786998748779
Mean loss is 0.0029085124001862685
-----------------------------------------------

RUN NUMBER 2
RUNNING FOR learning_rate: 0.01, num_units_1: 100, num_units_2: 30, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 9.59101676940918
step: 100, loss: 8.111201286315918
step: 200, loss: 5.695859909057617
step: 300, loss: 6.239103317260742
step: 400, loss: 5.383942604064941
step: 500, loss: 6.0851359367370605
step: 600, loss: 5.1165900230407715
step: 700, loss: 5.459061622619629
step: 800, loss: 5.5892415046691895
step: 900, loss: 4.607600212097168
step: 1000, loss: 4.847598075866699
step: 1100, loss: 4.470829963684082
step: 1200, loss: 4.8277387619018555
step: 1300, loss: 5.9295854568481445
step: 1400, loss: 7.040561199188232
step: 1500, loss: 4.496419906616211
step: 1600, loss: 5.190857887268066
step: 1700, loss: 5.059198379516602
step: 1800, loss: 4.615871429443359
step: 1900, loss: 5.649775505065918
step: 2000, loss: 3.8649890422821045
step: 2100, loss: 6.632962703704834
step: 2200, loss: 6.922386646270752
step: 2300, loss: 5.575960636138916
step: 2400, loss: 5.721689224243164
step: 2500, loss: 4.504786491394043
step: 2600, loss: 5.120228290557861
step: 2700, loss: 7.260727882385254
step: 2800, loss: 6.167849063873291
step: 2900, loss: 7.440805912017822
step: 3000, loss: 8.248185157775879
step: 3100, loss: 6.0026535987854
step: 3200, loss: 6.4061102867126465
step: 3300, loss: 5.067549705505371
step: 3400, loss: 6.1351213455200195
step: 3500, loss: 5.026669979095459
step: 3600, loss: 5.358940601348877
step: 3700, loss: 5.350527763366699
step: 3800, loss: 5.77011251449585
step: 3900, loss: 6.0598931312561035
step: 4000, loss: 6.4238762855529785
step: 4100, loss: 4.04180383682251
step: 4200, loss: 4.79927921295166
step: 4300, loss: 5.3270649909973145
step: 4400, loss: 4.875220775604248
step: 4500, loss: 4.265524387359619
step: 4600, loss: 4.861727237701416
step: 4700, loss: 6.436181545257568
step: 4800, loss: 6.834209442138672
step: 4900, loss: 5.467789649963379
step: 5000, loss: 6.059325218200684
step: 5100, loss: 4.532832145690918
step: 5200, loss: 5.147380828857422
step: 5300, loss: 5.751500129699707
step: 5400, loss: 4.355645656585693
step: 5500, loss: 5.564977169036865
step: 5600, loss: 5.319238662719727
step: 5700, loss: 6.177143096923828
step: 5800, loss: 6.759045600891113
step: 5900, loss: 5.1769700050354
step: 6000, loss: 4.911376476287842
step: 6100, loss: 5.2647705078125
step: 6200, loss: 4.491965293884277
step: 6300, loss: 7.045681953430176
step: 6400, loss: 6.310229301452637
step: 6500, loss: 6.50419807434082
step: 6600, loss: 5.305170059204102
step: 6700, loss: 4.6554155349731445
step: 6800, loss: 6.39346170425415
step: 6900, loss: 4.977468013763428
step: 7000, loss: 6.6681227684021
step: 7100, loss: 4.32689094543457
step: 7200, loss: 7.464637279510498
step: 7300, loss: 6.090226173400879
step: 7400, loss: 6.387948513031006
step: 7500, loss: 5.19444465637207
step: 7600, loss: 7.677557945251465
step: 7700, loss: 4.787067413330078
step: 7800, loss: 4.994913578033447
step: 7900, loss: 4.39114236831665
step: 8000, loss: 5.932265281677246
step: 8100, loss: 4.817923069000244
step: 8200, loss: 3.6899712085723877
step: 8300, loss: 4.6583476066589355
step: 8400, loss: 5.232094764709473
step: 8500, loss: 4.6282148361206055
step: 8600, loss: 7.6913533210754395
step: 8700, loss: 4.589524269104004
step: 8800, loss: 6.346627235412598
step: 8900, loss: 5.670499801635742
step: 9000, loss: 5.188394546508789
step: 9100, loss: 5.018359184265137
step: 9200, loss: 4.848210334777832
step: 9300, loss: 4.166682720184326
step: 9400, loss: 3.575179100036621
step: 9500, loss: 6.800323009490967
step: 9600, loss: 5.864439010620117
step: 9700, loss: 6.299048900604248
step: 9800, loss: 4.22696590423584
step: 9900, loss: 4.064949035644531
step: 10000, loss: 4.592846393585205
step: 10100, loss: 6.005194187164307
step: 10200, loss: 6.008354187011719
step: 10300, loss: 4.924361705780029
step: 10400, loss: 4.872172832489014
step: 10500, loss: 3.4842422008514404
step: 10600, loss: 5.154198169708252
step: 10700, loss: 6.003902912139893
step: 10800, loss: 7.818169593811035
step: 10900, loss: 7.3867573738098145
step: 11000, loss: 4.642041206359863
step: 11100, loss: 6.331412315368652
step: 11200, loss: 6.337220668792725
step: 11300, loss: 6.214385032653809
step: 11400, loss: 5.767874717712402
step: 11500, loss: 3.862370014190674
step: 11600, loss: 5.160884857177734
step: 11700, loss: 4.960435390472412
step: 11800, loss: 5.518959045410156
step: 11900, loss: 5.141343116760254
Mean loss is 0.0029016508401703213
-----------------------------------------------

RUN NUMBER 3
RUNNING FOR learning_rate: 0.01, num_units_1: 100, num_units_2: 40, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 10.594661712646484
step: 100, loss: 4.784143924713135
step: 200, loss: 7.507216453552246
step: 300, loss: 6.957435607910156
step: 400, loss: 4.953044414520264
step: 500, loss: 5.835743427276611
step: 600, loss: 6.126359939575195
step: 700, loss: 4.110650062561035
step: 800, loss: 5.79520320892334
step: 900, loss: 5.3138227462768555
step: 1000, loss: 7.108263969421387
step: 1100, loss: 5.215489387512207
step: 1200, loss: 6.855579376220703
step: 1300, loss: 4.605475902557373
step: 1400, loss: 6.511300563812256
step: 1500, loss: 4.983236789703369
step: 1600, loss: 6.958610534667969
step: 1700, loss: 4.612492561340332
step: 1800, loss: 4.115638732910156
step: 1900, loss: 6.2480692863464355
step: 2000, loss: 5.494594573974609
step: 2100, loss: 4.50923490524292
step: 2200, loss: 5.493167877197266
step: 2300, loss: 5.148653030395508
step: 2400, loss: 4.706249237060547
step: 2500, loss: 4.927270412445068
step: 2600, loss: 4.991093635559082
step: 2700, loss: 4.539761066436768
step: 2800, loss: 5.231529712677002
step: 2900, loss: 3.9020869731903076
step: 3000, loss: 5.011655807495117
step: 3100, loss: 5.251614570617676
step: 3200, loss: 5.276151657104492
step: 3300, loss: 4.151127338409424
step: 3400, loss: 6.25773811340332
step: 3500, loss: 6.619041442871094
step: 3600, loss: 5.337846755981445
step: 3700, loss: 7.508133888244629
step: 3800, loss: 6.004863262176514
step: 3900, loss: 6.7677435874938965
step: 4000, loss: 5.369556427001953
step: 4100, loss: 4.263465404510498
step: 4200, loss: 6.3621721267700195
step: 4300, loss: 6.450509071350098
step: 4400, loss: 4.154916286468506
step: 4500, loss: 5.052828311920166
step: 4600, loss: 4.566210746765137
step: 4700, loss: 5.519334316253662
step: 4800, loss: 5.473666667938232
step: 4900, loss: 5.027304649353027
step: 5000, loss: 4.7346510887146
step: 5100, loss: 5.882279396057129
step: 5200, loss: 4.479132175445557
step: 5300, loss: 5.488588809967041
step: 5400, loss: 5.63154935836792
step: 5500, loss: 5.064937591552734
step: 5600, loss: 5.55653715133667
step: 5700, loss: 5.566333293914795
step: 5800, loss: 5.6054863929748535
step: 5900, loss: 4.331885814666748
step: 6000, loss: 4.0534348487854
step: 6100, loss: 5.500734329223633
step: 6200, loss: 5.850761413574219
step: 6300, loss: 5.112318515777588
step: 6400, loss: 7.422565937042236
step: 6500, loss: 3.8354592323303223
step: 6600, loss: 4.498496055603027
step: 6700, loss: 5.915534973144531
step: 6800, loss: 6.4725847244262695
step: 6900, loss: 4.341679573059082
step: 7000, loss: 7.4940409660339355
step: 7100, loss: 6.227851867675781
step: 7200, loss: 5.124902248382568
step: 7300, loss: 5.879929065704346
step: 7400, loss: 4.942514896392822
step: 7500, loss: 3.846801280975342
step: 7600, loss: 6.5625901222229
step: 7700, loss: 7.6178812980651855
step: 7800, loss: 5.33407735824585
step: 7900, loss: 6.491366863250732
step: 8000, loss: 4.846296787261963
step: 8100, loss: 6.913756370544434
step: 8200, loss: 4.847764492034912
step: 8300, loss: 4.6826491355896
step: 8400, loss: 5.953225135803223
step: 8500, loss: 6.0394062995910645
step: 8600, loss: 5.810850620269775
step: 8700, loss: 5.9286651611328125
step: 8800, loss: 3.8684985637664795
step: 8900, loss: 5.696313381195068
step: 9000, loss: 5.3668904304504395
step: 9100, loss: 5.086830139160156
step: 9200, loss: 5.596347332000732
step: 9300, loss: 6.775789260864258
step: 9400, loss: 4.178354263305664
step: 9500, loss: 5.732151031494141
step: 9600, loss: 3.677670478820801
step: 9700, loss: 5.613832473754883
step: 9800, loss: 4.467119216918945
step: 9900, loss: 5.62998628616333
step: 10000, loss: 4.931734561920166
step: 10100, loss: 5.5064921379089355
step: 10200, loss: 4.433145046234131
step: 10300, loss: 5.078555107116699
step: 10400, loss: 5.88328742980957
step: 10500, loss: 4.6145782470703125
step: 10600, loss: 5.386926651000977
step: 10700, loss: 6.259528160095215
step: 10800, loss: 4.848511219024658
step: 10900, loss: 5.1923441886901855
step: 11000, loss: 5.10304594039917
step: 11100, loss: 5.6903581619262695
step: 11200, loss: 3.7634034156799316
step: 11300, loss: 6.1534857749938965
step: 11400, loss: 6.830313682556152
step: 11500, loss: 3.79301381111145
step: 11600, loss: 5.048901557922363
step: 11700, loss: 5.310966968536377
step: 11800, loss: 5.037200450897217
step: 11900, loss: 4.960615634918213
Mean loss is 0.0029050515058186583
-----------------------------------------------

RUN NUMBER 4
RUNNING FOR learning_rate: 0.01, num_units_1: 150, num_units_2: 20, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 8.90618896484375
step: 100, loss: 5.580523490905762
step: 200, loss: 7.177451133728027
step: 300, loss: 5.3862690925598145
step: 400, loss: 4.288450241088867
step: 500, loss: 5.466557025909424
step: 600, loss: 5.3839569091796875
step: 700, loss: 4.313571453094482
step: 800, loss: 4.431855201721191
step: 900, loss: 4.37239408493042
step: 1000, loss: 6.245743751525879
step: 1100, loss: 5.810706615447998
step: 1200, loss: 7.042907238006592
step: 1300, loss: 5.182682037353516
step: 1400, loss: 4.103778839111328
step: 1500, loss: 5.291945934295654
step: 1600, loss: 4.674757480621338
step: 1700, loss: 4.287843227386475
step: 1800, loss: 5.757059574127197
step: 1900, loss: 5.110482692718506
step: 2000, loss: 4.868030071258545
step: 2100, loss: 3.9407739639282227
step: 2200, loss: 6.395215034484863
step: 2300, loss: 3.887352228164673
step: 2400, loss: 4.601967811584473
step: 2500, loss: 5.041292667388916
step: 2600, loss: 5.879685878753662
step: 2700, loss: 5.5259928703308105
step: 2800, loss: 4.850170612335205
step: 2900, loss: 6.4138712882995605
step: 3000, loss: 6.399172306060791
step: 3100, loss: 6.066650390625
step: 3200, loss: 4.74033260345459
step: 3300, loss: 4.661069869995117
step: 3400, loss: 7.492798805236816
step: 3500, loss: 4.9259724617004395
step: 3600, loss: 5.543663501739502
step: 3700, loss: 5.050968647003174
step: 3800, loss: 5.491365432739258
step: 3900, loss: 5.238259315490723
step: 4000, loss: 5.547089099884033
step: 4100, loss: 4.378951549530029
step: 4200, loss: 4.912640571594238
step: 4300, loss: 4.407018184661865
step: 4400, loss: 5.55033540725708
step: 4500, loss: 5.009914875030518
step: 4600, loss: 6.8287177085876465
step: 4700, loss: 6.991352081298828
step: 4800, loss: 3.013720750808716
step: 4900, loss: 4.6690568923950195
step: 5000, loss: 6.026202201843262
step: 5100, loss: 5.643913269042969
step: 5200, loss: 6.16522216796875
step: 5300, loss: 5.493273735046387
step: 5400, loss: 4.313356399536133
step: 5500, loss: 4.723736763000488
step: 5600, loss: 5.702923774719238
step: 5700, loss: 6.102111339569092
step: 5800, loss: 6.404455661773682
step: 5900, loss: 5.922816753387451
step: 6000, loss: 5.585888385772705
step: 6100, loss: 4.6692585945129395
step: 6200, loss: 5.677846908569336
step: 6300, loss: 5.978952407836914
step: 6400, loss: 5.23472785949707
step: 6500, loss: 5.762359142303467
step: 6600, loss: 4.87364387512207
step: 6700, loss: 5.4102959632873535
step: 6800, loss: 4.614208698272705
step: 6900, loss: 5.839437484741211
step: 7000, loss: 5.142588138580322
step: 7100, loss: 4.687798023223877
step: 7200, loss: 5.698157787322998
step: 7300, loss: 4.9445481300354
step: 7400, loss: 6.626664638519287
step: 7500, loss: 6.513093948364258
step: 7600, loss: 6.685758590698242
step: 7700, loss: 5.938209533691406
step: 7800, loss: 4.889452934265137
step: 7900, loss: 6.385873317718506
step: 8000, loss: 8.230856895446777
step: 8100, loss: 6.831897735595703
step: 8200, loss: 6.976399898529053
step: 8300, loss: 6.344138145446777
step: 8400, loss: 4.7694573402404785
step: 8500, loss: 6.541492462158203
step: 8600, loss: 4.561524391174316
step: 8700, loss: 4.240028381347656
step: 8800, loss: 6.032420635223389
step: 8900, loss: 4.636892318725586
step: 9000, loss: 6.909007549285889
step: 9100, loss: 5.654597759246826
step: 9200, loss: 4.240104675292969
step: 9300, loss: 4.288299560546875
step: 9400, loss: 4.116781234741211
step: 9500, loss: 5.460982799530029
step: 9600, loss: 7.240444660186768
step: 9700, loss: 4.777271270751953
step: 9800, loss: 6.468084812164307
step: 9900, loss: 4.858389854431152
step: 10000, loss: 5.879610061645508
step: 10100, loss: 4.59459114074707
step: 10200, loss: 5.598930358886719
step: 10300, loss: 6.995786190032959
step: 10400, loss: 6.0937275886535645
step: 10500, loss: 6.351124286651611
step: 10600, loss: 5.948023796081543
step: 10700, loss: 6.119870185852051
step: 10800, loss: 6.075552940368652
step: 10900, loss: 8.3433198928833
step: 11000, loss: 5.086820602416992
step: 11100, loss: 7.167837142944336
step: 11200, loss: 5.3110671043396
step: 11300, loss: 5.2729668617248535
step: 11400, loss: 4.551468372344971
step: 11500, loss: 6.62337064743042
step: 11600, loss: 4.6782708168029785
step: 11700, loss: 4.613883972167969
step: 11800, loss: 4.159152030944824
step: 11900, loss: 4.721919059753418
Mean loss is 0.0029021838112154134
-----------------------------------------------

RUN NUMBER 5
RUNNING FOR learning_rate: 0.01, num_units_1: 150, num_units_2: 30, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 7.578647136688232
step: 100, loss: 6.667056560516357
step: 200, loss: 4.768576145172119
step: 300, loss: 6.581540584564209
step: 400, loss: 5.024272918701172
step: 500, loss: 5.440011978149414
step: 600, loss: 5.431576728820801
step: 700, loss: 5.738197326660156
step: 800, loss: 7.1089768409729
step: 900, loss: 4.723310947418213
step: 1000, loss: 5.00736665725708
step: 1100, loss: 5.9131035804748535
step: 1200, loss: 5.293708801269531
step: 1300, loss: 5.972837924957275
step: 1400, loss: 4.386035919189453
step: 1500, loss: 6.233944892883301
step: 1600, loss: 4.127397060394287
step: 1700, loss: 7.307531833648682
step: 1800, loss: 4.3384504318237305
step: 1900, loss: 5.968813896179199
step: 2000, loss: 6.000239849090576
step: 2100, loss: 5.7881059646606445
step: 2200, loss: 4.433422565460205
step: 2300, loss: 6.113874912261963
step: 2400, loss: 5.341361999511719
step: 2500, loss: 5.384740829467773
step: 2600, loss: 6.241782188415527
step: 2700, loss: 6.101161003112793
step: 2800, loss: 5.284193515777588
step: 2900, loss: 5.171341419219971
step: 3000, loss: 6.553905487060547
step: 3100, loss: 6.9968366622924805
step: 3200, loss: 4.946185111999512
step: 3300, loss: 7.331926345825195
step: 3400, loss: 6.329246520996094
step: 3500, loss: 6.326645374298096
step: 3600, loss: 4.782262325286865
step: 3700, loss: 7.149812698364258
step: 3800, loss: 4.16076135635376
step: 3900, loss: 7.182329177856445
step: 4000, loss: 5.881593227386475
step: 4100, loss: 5.0575761795043945
step: 4200, loss: 5.675908088684082
step: 4300, loss: 5.133405685424805
step: 4400, loss: 5.23598051071167
step: 4500, loss: 5.494421005249023
step: 4600, loss: 7.5355916023254395
step: 4700, loss: 5.634407043457031
step: 4800, loss: 6.689138412475586
step: 4900, loss: 6.381307601928711
step: 5000, loss: 4.395413398742676
step: 5100, loss: 5.33488655090332
step: 5200, loss: 5.555079936981201
step: 5300, loss: 5.215366363525391
step: 5400, loss: 4.7750773429870605
step: 5500, loss: 4.571038246154785
step: 5600, loss: 3.9324657917022705
step: 5700, loss: 4.517115116119385
step: 5800, loss: 4.443694591522217
step: 5900, loss: 4.89249324798584
step: 6000, loss: 6.649358749389648
step: 6100, loss: 5.47672176361084
step: 6200, loss: 4.673946857452393
step: 6300, loss: 6.250450611114502
step: 6400, loss: 5.092386245727539
step: 6500, loss: 4.3191633224487305
step: 6600, loss: 5.434935569763184
step: 6700, loss: 4.825188636779785
step: 6800, loss: 3.3196053504943848
step: 6900, loss: 6.021640300750732
step: 7000, loss: 4.592372417449951
step: 7100, loss: 5.477961540222168
step: 7200, loss: 5.849560260772705
step: 7300, loss: 4.5800652503967285
step: 7400, loss: 4.667660713195801
step: 7500, loss: 4.729434967041016
step: 7600, loss: 6.157291412353516
step: 7700, loss: 6.351006984710693
step: 7800, loss: 6.596098899841309
step: 7900, loss: 6.427600383758545
step: 8000, loss: 6.434377670288086
step: 8100, loss: 5.540060997009277
step: 8200, loss: 6.013029098510742
step: 8300, loss: 4.204606056213379
step: 8400, loss: 6.752080917358398
step: 8500, loss: 5.722679615020752
step: 8600, loss: 5.867715358734131
step: 8700, loss: 3.3035097122192383
step: 8800, loss: 5.114897727966309
step: 8900, loss: 4.531396865844727
step: 9000, loss: 4.4258623123168945
step: 9100, loss: 6.202480792999268
step: 9200, loss: 5.96694803237915
step: 9300, loss: 6.6090569496154785
step: 9400, loss: 5.705807685852051
step: 9500, loss: 4.972116470336914
step: 9600, loss: 6.644840240478516
step: 9700, loss: 6.517834663391113
step: 9800, loss: 3.1653013229370117
step: 9900, loss: 4.93656063079834
step: 10000, loss: 4.747730731964111
step: 10100, loss: 5.42082405090332
step: 10200, loss: 4.403456687927246
step: 10300, loss: 5.753359317779541
step: 10400, loss: 5.365793704986572
step: 10500, loss: 6.002084255218506
step: 10600, loss: 5.748185634613037
step: 10700, loss: 6.552667617797852
step: 10800, loss: 6.198737144470215
step: 10900, loss: 3.686997413635254
step: 11000, loss: 4.1866679191589355
step: 11100, loss: 5.197783470153809
step: 11200, loss: 5.093836307525635
step: 11300, loss: 6.692488193511963
step: 11400, loss: 5.763204574584961
step: 11500, loss: 6.539844512939453
step: 11600, loss: 4.982827186584473
step: 11700, loss: 6.18358039855957
step: 11800, loss: 4.751536846160889
step: 11900, loss: 5.822464466094971
Mean loss is 0.002902658884183796
-----------------------------------------------

RUN NUMBER 6
RUNNING FOR learning_rate: 0.01, num_units_1: 150, num_units_2: 40, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 9.645838737487793
step: 100, loss: 5.713769912719727
step: 200, loss: 5.5465569496154785
step: 300, loss: 5.868711948394775
step: 400, loss: 5.2686262130737305
step: 500, loss: 4.422996520996094
step: 600, loss: 6.379426956176758
step: 700, loss: 3.8821988105773926
step: 800, loss: 4.815843105316162
step: 900, loss: 5.524112701416016
step: 1000, loss: 4.509282112121582
step: 1100, loss: 3.603163003921509
step: 1200, loss: 5.268395900726318
step: 1300, loss: 5.099921703338623
step: 1400, loss: 5.790097236633301
step: 1500, loss: 5.173213005065918
step: 1600, loss: 5.1825971603393555
step: 1700, loss: 6.021238803863525
step: 1800, loss: 5.667984485626221
step: 1900, loss: 3.5179967880249023
step: 2000, loss: 6.299112319946289
step: 2100, loss: 5.025543689727783
step: 2200, loss: 6.237783908843994
step: 2300, loss: 4.710707664489746
step: 2400, loss: 4.303002834320068
step: 2500, loss: 3.541837692260742
step: 2600, loss: 4.641468048095703
step: 2700, loss: 4.3422088623046875
step: 2800, loss: 5.717416763305664
step: 2900, loss: 5.0739850997924805
step: 3000, loss: 5.5579328536987305
step: 3100, loss: 4.250816345214844
step: 3200, loss: 5.159756183624268
step: 3300, loss: 6.174698829650879
step: 3400, loss: 4.268027305603027
step: 3500, loss: 6.1784515380859375
step: 3600, loss: 5.408792972564697
step: 3700, loss: 3.5781147480010986
step: 3800, loss: 4.782246112823486
step: 3900, loss: 5.6809515953063965
step: 4000, loss: 5.052218914031982
step: 4100, loss: 4.130810737609863
step: 4200, loss: 5.662319660186768
step: 4300, loss: 8.581221580505371
step: 4400, loss: 4.368908882141113
step: 4500, loss: 5.380566596984863
step: 4600, loss: 5.76085901260376
step: 4700, loss: 6.565155506134033
step: 4800, loss: 5.126935958862305
step: 4900, loss: 5.682726860046387
step: 5000, loss: 5.664959907531738
step: 5100, loss: 6.064720630645752
step: 5200, loss: 5.834713459014893
step: 5300, loss: 6.611814022064209
step: 5400, loss: 5.120659351348877
step: 5500, loss: 6.307867050170898
step: 5600, loss: 5.36615514755249
step: 5700, loss: 3.622702121734619
step: 5800, loss: 5.781129360198975
step: 5900, loss: 5.707655906677246
step: 6000, loss: 5.2495198249816895
step: 6100, loss: 4.591314792633057
step: 6200, loss: 5.577535629272461
step: 6300, loss: 4.224928855895996
step: 6400, loss: 5.597131729125977
step: 6500, loss: 4.998589515686035
step: 6600, loss: 8.453826904296875
step: 6700, loss: 5.22517204284668
step: 6800, loss: 5.886131286621094
step: 6900, loss: 4.671206474304199
step: 7000, loss: 5.001712799072266
step: 7100, loss: 5.8015546798706055
step: 7200, loss: 4.201012134552002
step: 7300, loss: 5.271426200866699
step: 7400, loss: 4.354665756225586
step: 7500, loss: 5.594078540802002
step: 7600, loss: 8.216076850891113
step: 7700, loss: 8.486985206604004
step: 7800, loss: 5.643487930297852
step: 7900, loss: 6.30031681060791
step: 8000, loss: 5.320936679840088
step: 8100, loss: 6.2368011474609375
step: 8200, loss: 6.304168224334717
step: 8300, loss: 5.659340858459473
step: 8400, loss: 4.91859769821167
step: 8500, loss: 6.135377407073975
step: 8600, loss: 7.394194602966309
step: 8700, loss: 5.259259223937988
step: 8800, loss: 5.0203447341918945
step: 8900, loss: 6.794919013977051
step: 9000, loss: 4.615621566772461
step: 9100, loss: 6.211941719055176
step: 9200, loss: 3.790070056915283
step: 9300, loss: 5.482024669647217
step: 9400, loss: 4.8295674324035645
step: 9500, loss: 5.355324745178223
step: 9600, loss: 4.850246429443359
step: 9700, loss: 5.2059149742126465
step: 9800, loss: 6.316740989685059
step: 9900, loss: 6.130730152130127
step: 10000, loss: 6.291077136993408
step: 10100, loss: 6.534176826477051
step: 10200, loss: 4.437513828277588
step: 10300, loss: 4.921204566955566
step: 10400, loss: 4.688906192779541
step: 10500, loss: 4.907764434814453
step: 10600, loss: 4.375774383544922
step: 10700, loss: 6.590507507324219
step: 10800, loss: 5.061643123626709
step: 10900, loss: 4.368043422698975
step: 11000, loss: 5.606807231903076
step: 11100, loss: 6.479541301727295
step: 11200, loss: 5.116708278656006
step: 11300, loss: 4.747160911560059
step: 11400, loss: 5.93320369720459
step: 11500, loss: 5.026621341705322
step: 11600, loss: 5.267851829528809
step: 11700, loss: 5.866849899291992
step: 11800, loss: 6.626004695892334
step: 11900, loss: 5.077447891235352
Mean loss is 0.0029030659930561896
-----------------------------------------------

RUN NUMBER 7
RUNNING FOR learning_rate: 0.01, num_units_1: 200, num_units_2: 20, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 9.385119438171387
step: 100, loss: 5.837058067321777
step: 200, loss: 4.571532726287842
step: 300, loss: 6.908710479736328
step: 400, loss: 5.061844825744629
step: 500, loss: 5.3044114112854
step: 600, loss: 4.917018413543701
step: 700, loss: 5.228057861328125
step: 800, loss: 4.857847213745117
step: 900, loss: 4.077241897583008
step: 1000, loss: 5.6675872802734375
step: 1100, loss: 6.626781940460205
step: 1200, loss: 4.770620346069336
step: 1300, loss: 4.435340404510498
step: 1400, loss: 7.307112216949463
step: 1500, loss: 6.667783260345459
step: 1600, loss: 5.553399085998535
step: 1700, loss: 5.0575995445251465
step: 1800, loss: 6.323853492736816
step: 1900, loss: 5.322900295257568
step: 2000, loss: 6.504131317138672
step: 2100, loss: 5.49488639831543
step: 2200, loss: 6.600151538848877
step: 2300, loss: 4.426950931549072
step: 2400, loss: 5.757670879364014
step: 2500, loss: 4.962129592895508
step: 2600, loss: 5.611025333404541
step: 2700, loss: 5.294288158416748
step: 2800, loss: 5.417228698730469
step: 2900, loss: 5.439580917358398
step: 3000, loss: 3.8310868740081787
step: 3100, loss: 5.091912269592285
step: 3200, loss: 5.7770915031433105
step: 3300, loss: 7.366977691650391
step: 3400, loss: 4.7995100021362305
step: 3500, loss: 5.392351150512695
step: 3600, loss: 5.747204303741455
step: 3700, loss: 4.363605976104736
step: 3800, loss: 4.660110950469971
step: 3900, loss: 5.664923191070557
step: 4000, loss: 5.2416181564331055
step: 4100, loss: 6.560030937194824
step: 4200, loss: 5.809743404388428
step: 4300, loss: 4.735713481903076
step: 4400, loss: 7.011247158050537
step: 4500, loss: 4.825970649719238
step: 4600, loss: 5.5467143058776855
step: 4700, loss: 7.030773162841797
step: 4800, loss: 5.669523239135742
step: 4900, loss: 5.096085548400879
step: 5000, loss: 6.493477821350098
step: 5100, loss: 5.271954536437988
step: 5200, loss: 4.196233749389648
step: 5300, loss: 5.141132831573486
step: 5400, loss: 4.480039596557617
step: 5500, loss: 4.150248050689697
step: 5600, loss: 5.014429569244385
step: 5700, loss: 7.560081481933594
step: 5800, loss: 6.192877769470215
step: 5900, loss: 5.566798686981201
step: 6000, loss: 4.2026495933532715
step: 6100, loss: 5.326019763946533
step: 6200, loss: 6.166360855102539
step: 6300, loss: 5.834327697753906
step: 6400, loss: 3.8710594177246094
step: 6500, loss: 5.950173377990723
step: 6600, loss: 6.779503345489502
step: 6700, loss: 3.236358165740967
step: 6800, loss: 4.368262767791748
step: 6900, loss: 4.871581554412842
step: 7000, loss: 5.639068603515625
step: 7100, loss: 4.363962173461914
step: 7200, loss: 6.472798824310303
step: 7300, loss: 5.620222091674805
step: 7400, loss: 5.768274307250977
step: 7500, loss: 5.851633071899414
step: 7600, loss: 6.093265056610107
step: 7700, loss: 5.339041709899902
step: 7800, loss: 4.320347785949707
step: 7900, loss: 6.708848476409912
step: 8000, loss: 4.851868152618408
step: 8100, loss: 5.66148042678833
step: 8200, loss: 3.5589709281921387
step: 8300, loss: 4.226980209350586
step: 8400, loss: 5.457286834716797
step: 8500, loss: 5.446986198425293
step: 8600, loss: 4.905158042907715
step: 8700, loss: 5.677309036254883
step: 8800, loss: 6.7925004959106445
step: 8900, loss: 4.841119766235352
step: 9000, loss: 6.473965644836426
step: 9100, loss: 4.05924654006958
step: 9200, loss: 3.371570348739624
step: 9300, loss: 5.368724346160889
step: 9400, loss: 5.097352981567383
step: 9500, loss: 6.566518783569336
step: 9600, loss: 6.352962493896484
step: 9700, loss: 6.49994421005249
step: 9800, loss: 7.345461845397949
step: 9900, loss: 4.2840142250061035
step: 10000, loss: 3.855334758758545
step: 10100, loss: 4.805760860443115
step: 10200, loss: 4.546876430511475
step: 10300, loss: 6.721817493438721
step: 10400, loss: 4.854952812194824
step: 10500, loss: 5.634255886077881
step: 10600, loss: 4.400998592376709
step: 10700, loss: 4.857940673828125
step: 10800, loss: 7.074838161468506
step: 10900, loss: 4.1667304039001465
step: 11000, loss: 5.199489593505859
step: 11100, loss: 4.2809858322143555
step: 11200, loss: 4.653449535369873
step: 11300, loss: 4.465064525604248
step: 11400, loss: 5.726869583129883
step: 11500, loss: 5.419915676116943
step: 11600, loss: 5.165190696716309
step: 11700, loss: 5.140503406524658
step: 11800, loss: 5.121689319610596
step: 11900, loss: 5.148262977600098
Mean loss is 0.00290321328505585
-----------------------------------------------

RUN NUMBER 8
RUNNING FOR learning_rate: 0.01, num_units_1: 200, num_units_2: 30, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 11.407672882080078
step: 100, loss: 7.537798881530762
step: 200, loss: 4.891827583312988
step: 300, loss: 6.097939491271973
step: 400, loss: 6.109756946563721
step: 500, loss: 6.533702850341797
step: 600, loss: 5.772837162017822
step: 700, loss: 5.012739658355713
step: 800, loss: 6.563250541687012
step: 900, loss: 4.229459285736084
step: 1000, loss: 5.3009724617004395
step: 1100, loss: 4.94810152053833
step: 1200, loss: 7.5297136306762695
step: 1300, loss: 5.272603511810303
step: 1400, loss: 5.733520984649658
step: 1500, loss: 4.25246000289917
step: 1600, loss: 6.483132839202881
step: 1700, loss: 4.726189613342285
step: 1800, loss: 5.4273362159729
step: 1900, loss: 4.0494208335876465
step: 2000, loss: 5.955557823181152
step: 2100, loss: 5.813772201538086
step: 2200, loss: 5.038592338562012
step: 2300, loss: 6.031317234039307
step: 2400, loss: 6.0926361083984375
step: 2500, loss: 4.39476203918457
step: 2600, loss: 3.788971185684204
step: 2700, loss: 4.41210412979126
step: 2800, loss: 3.8724377155303955
step: 2900, loss: 5.946368217468262
step: 3000, loss: 5.756472110748291
step: 3100, loss: 6.889305591583252
step: 3200, loss: 6.896449565887451
step: 3300, loss: 4.352804183959961
step: 3400, loss: 5.405452251434326
step: 3500, loss: 4.296420574188232
step: 3600, loss: 4.474982261657715
step: 3700, loss: 4.683120250701904
step: 3800, loss: 7.339958190917969
step: 3900, loss: 6.656099796295166
step: 4000, loss: 6.317707538604736
step: 4100, loss: 4.80443811416626
step: 4200, loss: 5.077130317687988
step: 4300, loss: 3.951200008392334
step: 4400, loss: 5.9308576583862305
step: 4500, loss: 6.65947151184082
step: 4600, loss: 7.835692882537842
step: 4700, loss: 4.898550033569336
step: 4800, loss: 7.492486476898193
step: 4900, loss: 5.563203811645508
step: 5000, loss: 5.183167457580566
step: 5100, loss: 7.55417013168335
step: 5200, loss: 5.235440254211426
step: 5300, loss: 6.610841751098633
step: 5400, loss: 4.178432941436768
step: 5500, loss: 5.985541343688965
step: 5600, loss: 3.4376614093780518
step: 5700, loss: 4.784912109375
step: 5800, loss: 4.617076873779297
step: 5900, loss: 4.415034294128418
step: 6000, loss: 3.9126088619232178
step: 6100, loss: 3.7448954582214355
step: 6200, loss: 5.158707141876221
step: 6300, loss: 4.931190013885498
step: 6400, loss: 6.6227545738220215
step: 6500, loss: 6.0818586349487305
step: 6600, loss: 6.326257705688477
step: 6700, loss: 4.6057891845703125
step: 6800, loss: 5.2224836349487305
step: 6900, loss: 6.247674465179443
step: 7000, loss: 4.808041095733643
step: 7100, loss: 4.5675201416015625
step: 7200, loss: 4.595741271972656
step: 7300, loss: 4.895705699920654
step: 7400, loss: 5.571109771728516
step: 7500, loss: 6.412862777709961
step: 7600, loss: 5.130339622497559
step: 7700, loss: 5.303126335144043
step: 7800, loss: 4.269222259521484
step: 7900, loss: 4.37469482421875
step: 8000, loss: 5.692644119262695
step: 8100, loss: 5.676875114440918
step: 8200, loss: 6.654041290283203
step: 8300, loss: 6.740325927734375
step: 8400, loss: 4.756504535675049
step: 8500, loss: 7.901368141174316
step: 8600, loss: 5.3480000495910645
step: 8700, loss: 6.961373329162598
step: 8800, loss: 5.213440418243408
step: 8900, loss: 8.272233963012695
step: 9000, loss: 5.744407653808594
step: 9100, loss: 4.698836803436279
step: 9200, loss: 7.4156389236450195
step: 9300, loss: 7.936479091644287
step: 9400, loss: 5.958756923675537
step: 9500, loss: 3.9223318099975586
step: 9600, loss: 7.584897994995117
step: 9700, loss: 4.931499481201172
step: 9800, loss: 6.921488285064697
step: 9900, loss: 3.9727275371551514
step: 10000, loss: 3.920494556427002
step: 10100, loss: 6.267231464385986
step: 10200, loss: 6.957305431365967
step: 10300, loss: 5.322596073150635
step: 10400, loss: 6.630019664764404
step: 10500, loss: 8.188529014587402
step: 10600, loss: 4.366984844207764
step: 10700, loss: 7.597986698150635
step: 10800, loss: 4.357515811920166
step: 10900, loss: 7.152471542358398
step: 11000, loss: 7.437610626220703
step: 11100, loss: 6.783027648925781
step: 11200, loss: 5.495009422302246
step: 11300, loss: 6.2481184005737305
step: 11400, loss: 5.691211223602295
step: 11500, loss: 5.023960113525391
step: 11600, loss: 4.524765968322754
step: 11700, loss: 5.574469089508057
step: 11800, loss: 7.1995625495910645
step: 11900, loss: 6.268706321716309
Mean loss is 0.0029116516204002445
-----------------------------------------------

RUN NUMBER 9
RUNNING FOR learning_rate: 0.01, num_units_1: 200, num_units_2: 40, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 10.112991333007812
step: 100, loss: 5.767475128173828
step: 200, loss: 5.7318878173828125
step: 300, loss: 5.284934997558594
step: 400, loss: 5.821012496948242
step: 500, loss: 7.7208991050720215
step: 600, loss: 6.881666660308838
step: 700, loss: 4.577563285827637
step: 800, loss: 4.829322338104248
step: 900, loss: 6.509416580200195
step: 1000, loss: 4.276755332946777
step: 1100, loss: 5.8617048263549805
step: 1200, loss: 6.08742094039917
step: 1300, loss: 5.652163982391357
step: 1400, loss: 6.386338710784912
step: 1500, loss: 8.205785751342773
step: 1600, loss: 4.333493709564209
step: 1700, loss: 5.562193870544434
step: 1800, loss: 5.6865315437316895
step: 1900, loss: 8.060474395751953
step: 2000, loss: 5.279395580291748
step: 2100, loss: 4.614849090576172
step: 2200, loss: 4.986073017120361
step: 2300, loss: 4.866439342498779
step: 2400, loss: 4.956039905548096
step: 2500, loss: 4.582599639892578
step: 2600, loss: 5.758533477783203
step: 2700, loss: 6.382823944091797
step: 2800, loss: 4.862546920776367
step: 2900, loss: 4.516438961029053
step: 3000, loss: 4.6788482666015625
step: 3100, loss: 4.98503303527832
step: 3200, loss: 5.307801246643066
step: 3300, loss: 5.58411169052124
step: 3400, loss: 7.052997589111328
step: 3500, loss: 6.320652961730957
step: 3600, loss: 5.593053340911865
step: 3700, loss: 4.833014488220215
step: 3800, loss: 6.906131267547607
step: 3900, loss: 5.291897773742676
step: 4000, loss: 4.8481974601745605
step: 4100, loss: 6.155062675476074
step: 4200, loss: 5.777740001678467
step: 4300, loss: 3.792898654937744
step: 4400, loss: 7.198471546173096
step: 4500, loss: 5.714669704437256
step: 4600, loss: 6.421332359313965
step: 4700, loss: 7.347289562225342
step: 4800, loss: 5.100253582000732
step: 4900, loss: 5.507603645324707
step: 5000, loss: 6.124874114990234
step: 5100, loss: 4.857391357421875
step: 5200, loss: 4.877717971801758
step: 5300, loss: 5.492160797119141
step: 5400, loss: 4.924739837646484
step: 5500, loss: 6.445627689361572
step: 5600, loss: 4.344924449920654
step: 5700, loss: 5.0588788986206055
step: 5800, loss: 4.997452259063721
step: 5900, loss: 5.678005695343018
step: 6000, loss: 6.503124713897705
step: 6100, loss: 5.217237949371338
step: 6200, loss: 5.394010066986084
step: 6300, loss: 7.929169654846191
step: 6400, loss: 4.591525554656982
step: 6500, loss: 3.9517672061920166
step: 6600, loss: 4.776012420654297
step: 6700, loss: 6.01636266708374
step: 6800, loss: 4.952083110809326
step: 6900, loss: 5.8122687339782715
step: 7000, loss: 6.813523769378662
step: 7100, loss: 6.581540584564209
step: 7200, loss: 4.102435111999512
step: 7300, loss: 6.252933025360107
step: 7400, loss: 7.774140357971191
step: 7500, loss: 5.145236015319824
step: 7600, loss: 4.881714344024658
step: 7700, loss: 4.376781940460205
step: 7800, loss: 4.891281604766846
step: 7900, loss: 4.682206153869629
step: 8000, loss: 3.5911786556243896
step: 8100, loss: 5.574398517608643
step: 8200, loss: 4.644773006439209
step: 8300, loss: 4.094040393829346
step: 8400, loss: 4.7820844650268555
step: 8500, loss: 4.095490455627441
step: 8600, loss: 5.150598526000977
step: 8700, loss: 6.214052200317383
step: 8800, loss: 5.171538829803467
step: 8900, loss: 5.46335506439209
step: 9000, loss: 4.531531810760498
step: 9100, loss: 6.165193557739258
step: 9200, loss: 5.075807571411133
step: 9300, loss: 5.095719814300537
step: 9400, loss: 5.189157485961914
step: 9500, loss: 6.234631061553955
step: 9600, loss: 5.024501323699951
step: 9700, loss: 5.8355278968811035
step: 9800, loss: 5.8224382400512695
step: 9900, loss: 4.063521385192871
step: 10000, loss: 5.883642673492432
step: 10100, loss: 4.586912155151367
step: 10200, loss: 5.702165603637695
step: 10300, loss: 5.506155014038086
step: 10400, loss: 5.708704948425293
step: 10500, loss: 3.422955274581909
step: 10600, loss: 6.190506458282471
step: 10700, loss: 4.143082141876221
step: 10800, loss: 4.623096466064453
step: 10900, loss: 4.3241777420043945
step: 11000, loss: 6.021762371063232
step: 11100, loss: 5.017189979553223
step: 11200, loss: 7.39516544342041
step: 11300, loss: 6.345727443695068
step: 11400, loss: 5.86637020111084
step: 11500, loss: 4.857635974884033
step: 11600, loss: 5.714226245880127
step: 11700, loss: 5.116940021514893
step: 11800, loss: 5.065127372741699
step: 11900, loss: 5.323718547821045
Mean loss is 0.0029070897570645654
-----------------------------------------------

RUN NUMBER 10
RUNNING FOR learning_rate: 0.001, num_units_1: 100, num_units_2: 20, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 8.29886531829834
step: 100, loss: 7.98641300201416
step: 200, loss: 6.546664237976074
step: 300, loss: 6.505822658538818
step: 400, loss: 6.177955627441406
step: 500, loss: 6.2779860496521
step: 600, loss: 5.9628825187683105
step: 700, loss: 6.458710193634033
step: 800, loss: 5.960923194885254
step: 900, loss: 4.593830585479736
step: 1000, loss: 7.111781120300293
step: 1100, loss: 6.508691310882568
step: 1200, loss: 7.093094348907471
step: 1300, loss: 3.968393325805664
step: 1400, loss: 6.452475070953369
step: 1500, loss: 6.714751720428467
step: 1600, loss: 6.264168739318848
step: 1700, loss: 5.200225830078125
step: 1800, loss: 6.650001049041748
step: 1900, loss: 5.033526420593262
step: 2000, loss: 4.644270420074463
step: 2100, loss: 6.59993839263916
step: 2200, loss: 5.215579032897949
step: 2300, loss: 5.537568092346191
step: 2400, loss: 7.019263744354248
step: 2500, loss: 6.795969486236572
step: 2600, loss: 6.825403690338135
step: 2700, loss: 4.811691761016846
step: 2800, loss: 5.40474796295166
step: 2900, loss: 6.693207740783691
step: 3000, loss: 6.745145797729492
step: 3100, loss: 5.373840808868408
step: 3200, loss: 5.988393306732178
step: 3300, loss: 6.478240489959717
step: 3400, loss: 7.2113261222839355
step: 3500, loss: 4.9331889152526855
step: 3600, loss: 4.941501617431641
step: 3700, loss: 4.846309661865234
step: 3800, loss: 6.463864326477051
step: 3900, loss: 6.470852375030518
step: 4000, loss: 6.32095193862915
step: 4100, loss: 5.661288261413574
step: 4200, loss: 4.589923858642578
step: 4300, loss: 5.683380126953125
step: 4400, loss: 7.59287691116333
step: 4500, loss: 6.037088871002197
step: 4600, loss: 3.7935051918029785
step: 4700, loss: 5.1893510818481445
step: 4800, loss: 7.111209869384766
step: 4900, loss: 5.172055721282959
step: 5000, loss: 5.665361404418945
step: 5100, loss: 4.483775615692139
step: 5200, loss: 4.338176250457764
step: 5300, loss: 4.5752644538879395
step: 5400, loss: 6.252204895019531
step: 5500, loss: 4.85508918762207
step: 5600, loss: 5.923856735229492
step: 5700, loss: 5.385265827178955
step: 5800, loss: 4.542909622192383
step: 5900, loss: 4.499767780303955
step: 6000, loss: 4.425752639770508
step: 6100, loss: 5.6551690101623535
step: 6200, loss: 4.920073986053467
step: 6300, loss: 7.262515544891357
step: 6400, loss: 5.681417465209961
step: 6500, loss: 4.604761600494385
step: 6600, loss: 5.961064338684082
step: 6700, loss: 5.2788262367248535
step: 6800, loss: 6.534791469573975
step: 6900, loss: 8.7656888961792
step: 7000, loss: 5.140026569366455
step: 7100, loss: 4.243246078491211
step: 7200, loss: 5.260880470275879
step: 7300, loss: 7.235124588012695
step: 7400, loss: 5.041604518890381
step: 7500, loss: 6.035964012145996
step: 7600, loss: 6.487127304077148
step: 7700, loss: 6.153371810913086
step: 7800, loss: 5.274990558624268
step: 7900, loss: 5.1603217124938965
step: 8000, loss: 4.727931499481201
step: 8100, loss: 4.296243190765381
step: 8200, loss: 6.397530555725098
step: 8300, loss: 4.13044548034668
step: 8400, loss: 4.290682315826416
step: 8500, loss: 6.860252380371094
step: 8600, loss: 4.698954105377197
step: 8700, loss: 5.815783500671387
step: 8800, loss: 6.399394989013672
step: 8900, loss: 5.695927619934082
step: 9000, loss: 4.116798400878906
step: 9100, loss: 6.305492401123047
step: 9200, loss: 5.599302291870117
step: 9300, loss: 6.618409633636475
step: 9400, loss: 4.457702159881592
step: 9500, loss: 4.595340728759766
step: 9600, loss: 6.157528877258301
step: 9700, loss: 5.218578815460205
step: 9800, loss: 5.820283889770508
step: 9900, loss: 4.2432074546813965
step: 10000, loss: 4.744041442871094
step: 10100, loss: 5.90765380859375
step: 10200, loss: 6.669526100158691
step: 10300, loss: 4.336483478546143
step: 10400, loss: 4.098440170288086
step: 10500, loss: 5.246381759643555
step: 10600, loss: 5.845269680023193
step: 10700, loss: 4.786009788513184
step: 10800, loss: 5.300807476043701
step: 10900, loss: 5.024454593658447
step: 11000, loss: 5.544077396392822
step: 11100, loss: 5.813637733459473
step: 11200, loss: 6.611179828643799
step: 11300, loss: 5.832674503326416
step: 11400, loss: 6.137213230133057
step: 11500, loss: 5.4373250007629395
step: 11600, loss: 6.622249603271484
step: 11700, loss: 6.440474033355713
step: 11800, loss: 7.478353500366211
step: 11900, loss: 6.491426467895508
Mean loss is 0.0029416978377110837
-----------------------------------------------

RUN NUMBER 11
RUNNING FOR learning_rate: 0.001, num_units_1: 100, num_units_2: 30, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 8.591445922851562
step: 100, loss: 8.184470176696777
step: 200, loss: 8.524667739868164
step: 300, loss: 7.899023532867432
step: 400, loss: 7.414532661437988
step: 500, loss: 7.559580326080322
step: 600, loss: 6.334374904632568
step: 700, loss: 9.096128463745117
step: 800, loss: 7.955655574798584
step: 900, loss: 6.024550437927246
step: 1000, loss: 5.975529670715332
step: 1100, loss: 5.902618408203125
step: 1200, loss: 7.104788303375244
step: 1300, loss: 7.430636882781982
step: 1400, loss: 5.630946636199951
step: 1500, loss: 5.60621976852417
step: 1600, loss: 5.768619537353516
step: 1700, loss: 7.8169403076171875
step: 1800, loss: 6.5982985496521
step: 1900, loss: 5.576593399047852
step: 2000, loss: 6.265868186950684
step: 2100, loss: 6.1056294441223145
step: 2200, loss: 4.817802906036377
step: 2300, loss: 6.243826866149902
step: 2400, loss: 6.040310859680176
step: 2500, loss: 7.319620132446289
step: 2600, loss: 5.768472194671631
step: 2700, loss: 4.283266067504883
step: 2800, loss: 6.176585674285889
step: 2900, loss: 4.870443820953369
step: 3000, loss: 6.389304161071777
step: 3100, loss: 6.205522060394287
step: 3200, loss: 5.0637664794921875
step: 3300, loss: 6.277151107788086
step: 3400, loss: 6.3166680335998535
step: 3500, loss: 4.531881332397461
step: 3600, loss: 6.927309513092041
step: 3700, loss: 6.659930229187012
step: 3800, loss: 5.073840618133545
step: 3900, loss: 4.929346084594727
step: 4000, loss: 5.277958393096924
step: 4100, loss: 7.242170333862305
step: 4200, loss: 5.113452434539795
step: 4300, loss: 6.240973472595215
step: 4400, loss: 4.840600490570068
step: 4500, loss: 5.1420722007751465
step: 4600, loss: 6.041237831115723
step: 4700, loss: 6.740673065185547
step: 4800, loss: 5.691336631774902
step: 4900, loss: 5.208494663238525
step: 5000, loss: 4.846851348876953
step: 5100, loss: 5.731206893920898
step: 5200, loss: 4.755788803100586
step: 5300, loss: 6.03182315826416
step: 5400, loss: 6.9961724281311035
step: 5500, loss: 7.0461039543151855
step: 5600, loss: 4.753328800201416
step: 5700, loss: 7.044559001922607
step: 5800, loss: 4.374298572540283
step: 5900, loss: 6.206781387329102
step: 6000, loss: 5.240177154541016
step: 6100, loss: 4.809083938598633
step: 6200, loss: 7.833867073059082
step: 6300, loss: 4.727087020874023
step: 6400, loss: 5.462045669555664
step: 6500, loss: 6.179128170013428
step: 6600, loss: 5.997990131378174
step: 6700, loss: 4.785159111022949
step: 6800, loss: 4.062929153442383
step: 6900, loss: 5.718230247497559
step: 7000, loss: 4.871327877044678
step: 7100, loss: 4.578951835632324
step: 7200, loss: 5.708248615264893
step: 7300, loss: 6.4329423904418945
step: 7400, loss: 5.994697570800781
step: 7500, loss: 5.536118030548096
step: 7600, loss: 5.361263275146484
step: 7700, loss: 5.827666759490967
step: 7800, loss: 6.220727443695068
step: 7900, loss: 3.78365159034729
step: 8000, loss: 6.869443416595459
step: 8100, loss: 3.5122897624969482
step: 8200, loss: 5.091968536376953
step: 8300, loss: 4.43463659286499
step: 8400, loss: 4.458301067352295
step: 8500, loss: 5.330692291259766
step: 8600, loss: 5.56491231918335
step: 8700, loss: 6.540035247802734
step: 8800, loss: 4.10517692565918
step: 8900, loss: 5.953775882720947
step: 9000, loss: 5.080411434173584
step: 9100, loss: 6.128413200378418
step: 9200, loss: 5.500389099121094
step: 9300, loss: 4.644458293914795
step: 9400, loss: 5.752495765686035
step: 9500, loss: 4.556123733520508
step: 9600, loss: 4.8489179611206055
step: 9700, loss: 4.294132232666016
step: 9800, loss: 5.281457901000977
step: 9900, loss: 5.044450759887695
step: 10000, loss: 5.048966884613037
step: 10100, loss: 4.91565465927124
step: 10200, loss: 5.961169242858887
step: 10300, loss: 6.739323139190674
step: 10400, loss: 3.6471543312072754
step: 10500, loss: 6.738245010375977
step: 10600, loss: 5.907856464385986
step: 10700, loss: 5.489266872406006
step: 10800, loss: 4.8500165939331055
step: 10900, loss: 6.010762691497803
step: 11000, loss: 6.713590145111084
step: 11100, loss: 6.270305633544922
step: 11200, loss: 4.858053207397461
step: 11300, loss: 6.5074687004089355
step: 11400, loss: 4.541133403778076
step: 11500, loss: 4.813039779663086
step: 11600, loss: 7.918224334716797
step: 11700, loss: 6.394252777099609
step: 11800, loss: 4.838519096374512
step: 11900, loss: 4.2172322273254395
Mean loss is 0.0029430330524651107
-----------------------------------------------

RUN NUMBER 12
RUNNING FOR learning_rate: 0.001, num_units_1: 100, num_units_2: 40, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 10.053257942199707
step: 100, loss: 9.187170028686523
step: 200, loss: 8.771159172058105
step: 300, loss: 5.540429592132568
step: 400, loss: 8.014087677001953
step: 500, loss: 7.524179935455322
step: 600, loss: 6.861382961273193
step: 700, loss: 7.597962379455566
step: 800, loss: 8.98427963256836
step: 900, loss: 6.087064266204834
step: 1000, loss: 6.6328043937683105
step: 1100, loss: 4.46774435043335
step: 1200, loss: 6.53693962097168
step: 1300, loss: 5.586452007293701
step: 1400, loss: 5.925688743591309
step: 1500, loss: 5.446325302124023
step: 1600, loss: 6.343600749969482
step: 1700, loss: 5.757268905639648
step: 1800, loss: 6.5555901527404785
step: 1900, loss: 4.531181335449219
step: 2000, loss: 4.495353698730469
step: 2100, loss: 5.897924423217773
step: 2200, loss: 5.268847465515137
step: 2300, loss: 7.243781566619873
step: 2400, loss: 4.939637660980225
step: 2500, loss: 7.344629287719727
step: 2600, loss: 5.143150329589844
step: 2700, loss: 4.889636516571045
step: 2800, loss: 5.681912899017334
step: 2900, loss: 4.879351615905762
step: 3000, loss: 7.274921894073486
step: 3100, loss: 6.436753273010254
step: 3200, loss: 5.480821132659912
step: 3300, loss: 4.293837070465088
step: 3400, loss: 5.616437911987305
step: 3500, loss: 4.807679176330566
step: 3600, loss: 5.321770668029785
step: 3700, loss: 5.969629764556885
step: 3800, loss: 5.950892448425293
step: 3900, loss: 5.515652656555176
step: 4000, loss: 5.885503768920898
step: 4100, loss: 6.3557329177856445
step: 4200, loss: 5.0868964195251465
step: 4300, loss: 7.004087924957275
step: 4400, loss: 5.119858264923096
step: 4500, loss: 4.7157793045043945
step: 4600, loss: 5.926079750061035
step: 4700, loss: 5.7978644371032715
step: 4800, loss: 5.7853546142578125
step: 4900, loss: 5.822286128997803
step: 5000, loss: 5.272354602813721
step: 5100, loss: 5.261554718017578
step: 5200, loss: 6.227965354919434
step: 5300, loss: 4.9421515464782715
step: 5400, loss: 5.133342266082764
step: 5500, loss: 6.44623327255249
step: 5600, loss: 6.012071132659912
step: 5700, loss: 4.997452735900879
step: 5800, loss: 4.948784351348877
step: 5900, loss: 5.254267692565918
step: 6000, loss: 4.60804557800293
step: 6100, loss: 5.12318754196167
step: 6200, loss: 5.005062580108643
step: 6300, loss: 6.366635322570801
step: 6400, loss: 4.763426303863525
step: 6500, loss: 5.920899391174316
step: 6600, loss: 5.408196449279785
step: 6700, loss: 5.650216102600098
step: 6800, loss: 6.917520999908447
step: 6900, loss: 5.682194232940674
step: 7000, loss: 5.973789691925049
step: 7100, loss: 5.529693126678467
step: 7200, loss: 6.035686492919922
step: 7300, loss: 6.367014408111572
step: 7400, loss: 4.9119062423706055
step: 7500, loss: 5.344187259674072
step: 7600, loss: 6.162557125091553
step: 7700, loss: 6.713991641998291
step: 7800, loss: 5.2869486808776855
step: 7900, loss: 6.0651092529296875
step: 8000, loss: 4.6488728523254395
step: 8100, loss: 6.856868743896484
step: 8200, loss: 4.511039733886719
step: 8300, loss: 4.660050392150879
step: 8400, loss: 5.815138816833496
step: 8500, loss: 5.560515403747559
step: 8600, loss: 5.399951457977295
step: 8700, loss: 5.228109359741211
step: 8800, loss: 5.895707607269287
step: 8900, loss: 5.899077415466309
step: 9000, loss: 6.1275153160095215
step: 9100, loss: 5.961399555206299
step: 9200, loss: 6.029587745666504
step: 9300, loss: 4.471562385559082
step: 9400, loss: 4.242447853088379
step: 9500, loss: 6.285369873046875
step: 9600, loss: 4.078770160675049
step: 9700, loss: 4.566829204559326
step: 9800, loss: 5.247313499450684
step: 9900, loss: 5.247932434082031
step: 10000, loss: 6.413600921630859
step: 10100, loss: 5.876307010650635
step: 10200, loss: 4.592681884765625
step: 10300, loss: 3.789762258529663
step: 10400, loss: 5.833250045776367
step: 10500, loss: 5.957614898681641
step: 10600, loss: 5.841240882873535
step: 10700, loss: 5.8671746253967285
step: 10800, loss: 5.932015895843506
step: 10900, loss: 5.436616897583008
step: 11000, loss: 5.327241897583008
step: 11100, loss: 5.808875560760498
step: 11200, loss: 5.737844467163086
step: 11300, loss: 5.068628311157227
step: 11400, loss: 4.852710247039795
step: 11500, loss: 7.404191493988037
step: 11600, loss: 5.613503456115723
step: 11700, loss: 4.7756028175354
step: 11800, loss: 4.713279724121094
step: 11900, loss: 5.97764778137207
Mean loss is 0.002943242654735893
-----------------------------------------------

RUN NUMBER 13
RUNNING FOR learning_rate: 0.001, num_units_1: 150, num_units_2: 20, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 8.179465293884277
step: 100, loss: 6.0326972007751465
step: 200, loss: 9.571623802185059
step: 300, loss: 7.280035018920898
step: 400, loss: 6.612922668457031
step: 500, loss: 6.333436965942383
step: 600, loss: 9.078125953674316
step: 700, loss: 7.273889064788818
step: 800, loss: 7.5051445960998535
step: 900, loss: 7.140077590942383
step: 1000, loss: 6.240403175354004
step: 1100, loss: 6.148962020874023
step: 1200, loss: 5.282639503479004
step: 1300, loss: 5.350739479064941
step: 1400, loss: 7.167459011077881
step: 1500, loss: 6.594787120819092
step: 1600, loss: 6.747956275939941
step: 1700, loss: 6.542657852172852
step: 1800, loss: 5.013399124145508
step: 1900, loss: 6.013926029205322
step: 2000, loss: 5.135966777801514
step: 2100, loss: 7.35574197769165
step: 2200, loss: 5.876540660858154
step: 2300, loss: 5.285533428192139
step: 2400, loss: 7.359873294830322
step: 2500, loss: 6.3059468269348145
step: 2600, loss: 6.115809440612793
step: 2700, loss: 7.820762634277344
step: 2800, loss: 4.499913692474365
step: 2900, loss: 7.65730094909668
step: 3000, loss: 5.32520866394043
step: 3100, loss: 6.311413288116455
step: 3200, loss: 6.016726970672607
step: 3300, loss: 5.187524318695068
step: 3400, loss: 5.6320953369140625
step: 3500, loss: 4.6578288078308105
step: 3600, loss: 4.408143520355225
step: 3700, loss: 4.7775750160217285
step: 3800, loss: 7.53491735458374
step: 3900, loss: 6.4040350914001465
step: 4000, loss: 5.161623954772949
step: 4100, loss: 6.666791915893555
step: 4200, loss: 6.3699870109558105
step: 4300, loss: 5.3596110343933105
step: 4400, loss: 6.38972806930542
step: 4500, loss: 5.990309715270996
step: 4600, loss: 5.207675933837891
step: 4700, loss: 3.7470881938934326
step: 4800, loss: 5.783107757568359
step: 4900, loss: 4.942431449890137
step: 5000, loss: 9.1254301071167
step: 5100, loss: 4.569068908691406
step: 5200, loss: 6.219411849975586
step: 5300, loss: 7.009861946105957
step: 5400, loss: 4.884957313537598
step: 5500, loss: 6.464452266693115
step: 5600, loss: 4.983476638793945
step: 5700, loss: 4.792849063873291
step: 5800, loss: 4.718493938446045
step: 5900, loss: 6.824321746826172
step: 6000, loss: 5.494348049163818
step: 6100, loss: 5.939344882965088
step: 6200, loss: 4.776729583740234
step: 6300, loss: 5.344094276428223
step: 6400, loss: 6.268604278564453
step: 6500, loss: 6.290355682373047
step: 6600, loss: 6.777982711791992
step: 6700, loss: 5.573882102966309
step: 6800, loss: 5.0329155921936035
step: 6900, loss: 9.238510131835938
step: 7000, loss: 6.550900936126709
step: 7100, loss: 7.066414833068848
step: 7200, loss: 7.1283392906188965
step: 7300, loss: 5.885932445526123
step: 7400, loss: 5.504169940948486
step: 7500, loss: 3.589327096939087
step: 7600, loss: 5.323645114898682
step: 7700, loss: 5.583278179168701
step: 7800, loss: 5.086361885070801
step: 7900, loss: 4.521407127380371
step: 8000, loss: 6.181079864501953
step: 8100, loss: 4.525697231292725
step: 8200, loss: 5.472589492797852
step: 8300, loss: 4.2032623291015625
step: 8400, loss: 7.471471309661865
step: 8500, loss: 5.324950695037842
step: 8600, loss: 5.379818916320801
step: 8700, loss: 6.390352249145508
step: 8800, loss: 4.580441951751709
step: 8900, loss: 6.205227375030518
step: 9000, loss: 4.637729167938232
step: 9100, loss: 4.730135440826416
step: 9200, loss: 4.766576766967773
step: 9300, loss: 5.575969696044922
step: 9400, loss: 5.227653503417969
step: 9500, loss: 4.409583568572998
step: 9600, loss: 4.769204616546631
step: 9700, loss: 4.887058734893799
step: 9800, loss: 6.537353992462158
step: 9900, loss: 6.400947093963623
step: 10000, loss: 3.254324197769165
step: 10100, loss: 4.252828598022461
step: 10200, loss: 6.002472400665283
step: 10300, loss: 5.5232648849487305
step: 10400, loss: 4.0584330558776855
step: 10500, loss: 7.91630220413208
step: 10600, loss: 4.775641918182373
step: 10700, loss: 5.522977828979492
step: 10800, loss: 6.330643177032471
step: 10900, loss: 5.3984761238098145
step: 11000, loss: 5.809620380401611
step: 11100, loss: 4.673305988311768
step: 11200, loss: 5.1533284187316895
step: 11300, loss: 4.770135402679443
step: 11400, loss: 4.2134270668029785
step: 11500, loss: 6.54138708114624
step: 11600, loss: 7.650007724761963
step: 11700, loss: 6.5584869384765625
step: 11800, loss: 3.6871938705444336
step: 11900, loss: 5.096319675445557
Mean loss is 0.002944338488508681
-----------------------------------------------

RUN NUMBER 14
RUNNING FOR learning_rate: 0.001, num_units_1: 150, num_units_2: 30, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 10.04028606414795
step: 100, loss: 9.761865615844727
step: 200, loss: 11.38211727142334
step: 300, loss: 5.8821635246276855
step: 400, loss: 6.713183403015137
step: 500, loss: 7.386283874511719
step: 600, loss: 8.03219985961914
step: 700, loss: 7.169442176818848
step: 800, loss: 6.829901695251465
step: 900, loss: 6.07699728012085
step: 1000, loss: 6.816751956939697
step: 1100, loss: 6.091489315032959
step: 1200, loss: 7.34862756729126
step: 1300, loss: 5.207699298858643
step: 1400, loss: 5.902655601501465
step: 1500, loss: 7.337752819061279
step: 1600, loss: 5.981360912322998
step: 1700, loss: 6.886314392089844
step: 1800, loss: 4.860795974731445
step: 1900, loss: 6.060122966766357
step: 2000, loss: 4.567712783813477
step: 2100, loss: 7.1199798583984375
step: 2200, loss: 5.384570598602295
step: 2300, loss: 5.464541435241699
step: 2400, loss: 7.767698287963867
step: 2500, loss: 5.6735100746154785
step: 2600, loss: 5.234855651855469
step: 2700, loss: 4.8187665939331055
step: 2800, loss: 6.2943196296691895
step: 2900, loss: 7.139777660369873
step: 3000, loss: 6.820534706115723
step: 3100, loss: 7.119359016418457
step: 3200, loss: 7.2368574142456055
step: 3300, loss: 6.0156683921813965
step: 3400, loss: 6.337596893310547
step: 3500, loss: 5.234560966491699
step: 3600, loss: 5.793793678283691
step: 3700, loss: 5.894773960113525
step: 3800, loss: 5.959996700286865
step: 3900, loss: 5.068648815155029
step: 4000, loss: 4.806170463562012
step: 4100, loss: 5.15887451171875
step: 4200, loss: 7.419662952423096
step: 4300, loss: 4.628376007080078
step: 4400, loss: 6.193615913391113
step: 4500, loss: 5.630855560302734
step: 4600, loss: 3.2078959941864014
step: 4700, loss: 7.93461275100708
step: 4800, loss: 6.1503071784973145
step: 4900, loss: 6.877659797668457
step: 5000, loss: 6.413394927978516
step: 5100, loss: 6.204663276672363
step: 5200, loss: 6.848598957061768
step: 5300, loss: 6.616193771362305
step: 5400, loss: 6.54746675491333
step: 5500, loss: 6.8364644050598145
step: 5600, loss: 5.786746978759766
step: 5700, loss: 8.391829490661621
step: 5800, loss: 5.9044623374938965
step: 5900, loss: 4.485721588134766
step: 6000, loss: 5.997213840484619
step: 6100, loss: 5.907413005828857
step: 6200, loss: 6.17612886428833
step: 6300, loss: 5.5572285652160645
step: 6400, loss: 5.998265743255615
step: 6500, loss: 5.1592559814453125
step: 6600, loss: 6.056083679199219
step: 6700, loss: 6.089832782745361
step: 6800, loss: 5.5664567947387695
step: 6900, loss: 5.386592864990234
step: 7000, loss: 5.665425777435303
step: 7100, loss: 5.190401077270508
step: 7200, loss: 4.415892124176025
step: 7300, loss: 4.619032382965088
step: 7400, loss: 5.486478805541992
step: 7500, loss: 4.590333938598633
step: 7600, loss: 6.298954010009766
step: 7700, loss: 5.085308074951172
step: 7800, loss: 4.611801624298096
step: 7900, loss: 5.3600335121154785
step: 8000, loss: 6.205705165863037
step: 8100, loss: 5.418984889984131
step: 8200, loss: 4.761168479919434
step: 8300, loss: 6.220032691955566
step: 8400, loss: 6.659329891204834
step: 8500, loss: 6.225686073303223
step: 8600, loss: 6.093581199645996
step: 8700, loss: 4.748253345489502
step: 8800, loss: 5.764224052429199
step: 8900, loss: 5.471902847290039
step: 9000, loss: 6.290144443511963
step: 9100, loss: 5.392876625061035
step: 9200, loss: 5.088418006896973
step: 9300, loss: 7.024135589599609
step: 9400, loss: 4.8370537757873535
step: 9500, loss: 5.771059036254883
step: 9600, loss: 5.5262932777404785
step: 9700, loss: 5.989515781402588
step: 9800, loss: 4.839168071746826
step: 9900, loss: 6.007978439331055
step: 10000, loss: 6.618740558624268
step: 10100, loss: 5.167482852935791
step: 10200, loss: 6.097529888153076
step: 10300, loss: 5.604897499084473
step: 10400, loss: 5.574643611907959
step: 10500, loss: 6.058444976806641
step: 10600, loss: 3.769061326980591
step: 10700, loss: 5.86969518661499
step: 10800, loss: 5.766488552093506
step: 10900, loss: 5.857227802276611
step: 11000, loss: 5.890444755554199
step: 11100, loss: 5.673827171325684
step: 11200, loss: 5.232602119445801
step: 11300, loss: 5.953194618225098
step: 11400, loss: 6.069441318511963
step: 11500, loss: 3.4942495822906494
step: 11600, loss: 5.490428924560547
step: 11700, loss: 4.037599563598633
step: 11800, loss: 6.278741359710693
step: 11900, loss: 4.936315059661865
Mean loss is 0.0029435876161551067
-----------------------------------------------

RUN NUMBER 15
RUNNING FOR learning_rate: 0.001, num_units_1: 150, num_units_2: 40, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 9.879590034484863
step: 100, loss: 9.24523639678955
step: 200, loss: 7.046124458312988
step: 300, loss: 8.195191383361816
step: 400, loss: 8.287981986999512
step: 500, loss: 6.653185844421387
step: 600, loss: 7.943228244781494
step: 700, loss: 5.888489723205566
step: 800, loss: 5.935714244842529
step: 900, loss: 6.487118244171143
step: 1000, loss: 7.056783676147461
step: 1100, loss: 7.096709728240967
step: 1200, loss: 4.805933952331543
step: 1300, loss: 6.228344917297363
step: 1400, loss: 6.869109153747559
step: 1500, loss: 6.256801128387451
step: 1600, loss: 5.963951110839844
step: 1700, loss: 6.041641712188721
step: 1800, loss: 5.652071952819824
step: 1900, loss: 6.539985656738281
step: 2000, loss: 7.518620491027832
step: 2100, loss: 6.848941802978516
step: 2200, loss: 3.972043752670288
step: 2300, loss: 3.92277455329895
step: 2400, loss: 6.151608467102051
step: 2500, loss: 7.406110763549805
step: 2600, loss: 5.034268856048584
step: 2700, loss: 5.3823466300964355
step: 2800, loss: 7.054626941680908
step: 2900, loss: 6.591148853302002
step: 3000, loss: 5.923814296722412
step: 3100, loss: 5.805640697479248
step: 3200, loss: 5.9743266105651855
step: 3300, loss: 4.425817012786865
step: 3400, loss: 5.882174491882324
step: 3500, loss: 7.822054862976074
step: 3600, loss: 6.5113677978515625
step: 3700, loss: 6.066850185394287
step: 3800, loss: 5.137387752532959
step: 3900, loss: 4.775488376617432
step: 4000, loss: 5.05526876449585
step: 4100, loss: 6.492793083190918
step: 4200, loss: 8.796438217163086
step: 4300, loss: 4.77330207824707
step: 4400, loss: 6.088631629943848
step: 4500, loss: 4.292956352233887
step: 4600, loss: 8.067229270935059
step: 4700, loss: 6.145153522491455
step: 4800, loss: 5.782344818115234
step: 4900, loss: 6.037718772888184
step: 5000, loss: 5.019629001617432
step: 5100, loss: 6.108325004577637
step: 5200, loss: 5.355428695678711
step: 5300, loss: 5.38217830657959
step: 5400, loss: 4.329348087310791
step: 5500, loss: 5.320985794067383
step: 5600, loss: 6.39672327041626
step: 5700, loss: 4.092360019683838
step: 5800, loss: 4.99826717376709
step: 5900, loss: 5.769150733947754
step: 6000, loss: 6.909078598022461
step: 6100, loss: 5.864751815795898
step: 6200, loss: 5.589667320251465
step: 6300, loss: 5.376522541046143
step: 6400, loss: 5.527489185333252
step: 6500, loss: 5.193210124969482
step: 6600, loss: 5.358316421508789
step: 6700, loss: 5.606196880340576
step: 6800, loss: 6.35208797454834
step: 6900, loss: 6.073678970336914
step: 7000, loss: 5.801517486572266
step: 7100, loss: 3.5779435634613037
step: 7200, loss: 5.846754550933838
step: 7300, loss: 6.616032123565674
step: 7400, loss: 7.459223747253418
step: 7500, loss: 6.97460412979126
step: 7600, loss: 6.0596232414245605
step: 7700, loss: 5.316247940063477
step: 7800, loss: 5.546228408813477
step: 7900, loss: 5.500217914581299
step: 8000, loss: 4.27887487411499
step: 8100, loss: 4.893099784851074
step: 8200, loss: 6.626201152801514
step: 8300, loss: 5.540107250213623
step: 8400, loss: 5.363165855407715
step: 8500, loss: 4.884336948394775
step: 8600, loss: 5.586921691894531
step: 8700, loss: 5.3653388023376465
step: 8800, loss: 5.163084030151367
step: 8900, loss: 5.453695297241211
step: 9000, loss: 5.684986591339111
step: 9100, loss: 5.454434394836426
step: 9200, loss: 4.635293483734131
step: 9300, loss: 6.134676456451416
step: 9400, loss: 6.430623531341553
step: 9500, loss: 5.31996488571167
step: 9600, loss: 5.120244979858398
step: 9700, loss: 8.511788368225098
step: 9800, loss: 4.794501304626465
step: 9900, loss: 3.8898873329162598
step: 10000, loss: 7.188189506530762
step: 10100, loss: 5.57147216796875
step: 10200, loss: 4.50621223449707
step: 10300, loss: 5.4251298904418945
step: 10400, loss: 5.334679126739502
step: 10500, loss: 4.506096363067627
step: 10600, loss: 6.148117542266846
step: 10700, loss: 5.1184797286987305
step: 10800, loss: 5.344817638397217
step: 10900, loss: 7.235776424407959
step: 11000, loss: 5.256494998931885
step: 11100, loss: 5.497359752655029
step: 11200, loss: 4.697494983673096
step: 11300, loss: 4.241765022277832
step: 11400, loss: 5.612329959869385
step: 11500, loss: 5.680805683135986
step: 11600, loss: 5.074060916900635
step: 11700, loss: 6.16662073135376
step: 11800, loss: 6.009923458099365
step: 11900, loss: 6.046910285949707
Mean loss is 0.0029393068115948955
-----------------------------------------------

RUN NUMBER 16
RUNNING FOR learning_rate: 0.001, num_units_1: 200, num_units_2: 20, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 9.331750869750977
step: 100, loss: 8.474475860595703
step: 200, loss: 5.940301418304443
step: 300, loss: 8.148571968078613
step: 400, loss: 6.064985275268555
step: 500, loss: 6.595814228057861
step: 600, loss: 8.597827911376953
step: 700, loss: 6.798880100250244
step: 800, loss: 8.517416000366211
step: 900, loss: 9.194211959838867
step: 1000, loss: 5.781923770904541
step: 1100, loss: 5.446751117706299
step: 1200, loss: 7.2972517013549805
step: 1300, loss: 8.061782836914062
step: 1400, loss: 6.955038070678711
step: 1500, loss: 5.973180294036865
step: 1600, loss: 6.401655673980713
step: 1700, loss: 6.32036018371582
step: 1800, loss: 8.198390007019043
step: 1900, loss: 7.753131866455078
step: 2000, loss: 7.09946870803833
step: 2100, loss: 6.453298568725586
step: 2200, loss: 7.5327887535095215
step: 2300, loss: 6.251438140869141
step: 2400, loss: 6.211773872375488
step: 2500, loss: 5.054243564605713
step: 2600, loss: 5.957572937011719
step: 2700, loss: 6.088288307189941
step: 2800, loss: 5.8443145751953125
step: 2900, loss: 7.574881076812744
step: 3000, loss: 6.365067481994629
step: 3100, loss: 6.577408313751221
step: 3200, loss: 5.143208026885986
step: 3300, loss: 6.377063274383545
step: 3400, loss: 4.878542423248291
step: 3500, loss: 4.6427903175354
step: 3600, loss: 5.30490255355835
step: 3700, loss: 5.774806976318359
step: 3800, loss: 6.578131675720215
step: 3900, loss: 4.825422286987305
step: 4000, loss: 4.877574443817139
step: 4100, loss: 4.983935356140137
step: 4200, loss: 7.121509552001953
step: 4300, loss: 5.085068225860596
step: 4400, loss: 4.7210211753845215
step: 4500, loss: 4.483876705169678
step: 4600, loss: 7.230460166931152
step: 4700, loss: 4.557711124420166
step: 4800, loss: 4.8994317054748535
step: 4900, loss: 5.277135848999023
step: 5000, loss: 6.357097625732422
step: 5100, loss: 7.076604843139648
step: 5200, loss: 5.805800437927246
step: 5300, loss: 7.3825860023498535
step: 5400, loss: 4.848973274230957
step: 5500, loss: 7.622059345245361
step: 5600, loss: 5.314864635467529
step: 5700, loss: 4.993063926696777
step: 5800, loss: 5.303243637084961
step: 5900, loss: 5.013806343078613
step: 6000, loss: 5.319170951843262
step: 6100, loss: 6.218481063842773
step: 6200, loss: 5.502908229827881
step: 6300, loss: 6.398465633392334
step: 6400, loss: 4.640249729156494
step: 6500, loss: 6.9354400634765625
step: 6600, loss: 5.695516586303711
step: 6700, loss: 4.7702860832214355
step: 6800, loss: 5.783939361572266
step: 6900, loss: 7.606081485748291
step: 7000, loss: 5.439618110656738
step: 7100, loss: 6.18745231628418
step: 7200, loss: 3.9757375717163086
step: 7300, loss: 6.246256351470947
step: 7400, loss: 5.579569339752197
step: 7500, loss: 4.774013042449951
step: 7600, loss: 6.854572772979736
step: 7700, loss: 5.005893707275391
step: 7800, loss: 6.911957263946533
step: 7900, loss: 5.547317028045654
step: 8000, loss: 4.502546787261963
step: 8100, loss: 5.840692520141602
step: 8200, loss: 7.369576454162598
step: 8300, loss: 5.639246463775635
step: 8400, loss: 3.510500907897949
step: 8500, loss: 5.136849880218506
step: 8600, loss: 3.7923531532287598
step: 8700, loss: 5.77339506149292
step: 8800, loss: 6.044836521148682
step: 8900, loss: 6.5687079429626465
step: 9000, loss: 6.100021839141846
step: 9100, loss: 5.1906538009643555
step: 9200, loss: 6.4484782218933105
step: 9300, loss: 4.714008331298828
step: 9400, loss: 5.7139692306518555
step: 9500, loss: 4.686075210571289
step: 9600, loss: 5.936744213104248
step: 9700, loss: 5.4782562255859375
step: 9800, loss: 7.278197765350342
step: 9900, loss: 4.978909015655518
step: 10000, loss: 4.298330783843994
step: 10100, loss: 6.240853309631348
step: 10200, loss: 5.137351989746094
step: 10300, loss: 4.805678367614746
step: 10400, loss: 7.424701690673828
step: 10500, loss: 4.457756519317627
step: 10600, loss: 5.8897528648376465
step: 10700, loss: 5.272679328918457
step: 10800, loss: 5.30014705657959
step: 10900, loss: 8.699003219604492
step: 11000, loss: 7.0391411781311035
step: 11100, loss: 5.215436935424805
step: 11200, loss: 4.217740058898926
step: 11300, loss: 7.241064548492432
step: 11400, loss: 6.279315948486328
step: 11500, loss: 5.779805660247803
step: 11600, loss: 5.257773399353027
step: 11700, loss: 5.90452766418457
step: 11800, loss: 6.565954208374023
step: 11900, loss: 3.744565486907959
Mean loss is 0.002946943863469297
-----------------------------------------------

RUN NUMBER 17
RUNNING FOR learning_rate: 0.001, num_units_1: 200, num_units_2: 30, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 11.350282669067383
step: 100, loss: 7.367690086364746
step: 200, loss: 8.481399536132812
step: 300, loss: 5.267241954803467
step: 400, loss: 8.20592975616455
step: 500, loss: 7.258799076080322
step: 600, loss: 8.376232147216797
step: 700, loss: 8.396615982055664
step: 800, loss: 5.5590715408325195
step: 900, loss: 4.846660614013672
step: 1000, loss: 5.277492046356201
step: 1100, loss: 8.329840660095215
step: 1200, loss: 7.105653285980225
step: 1300, loss: 5.7890625
step: 1400, loss: 6.811187267303467
step: 1500, loss: 5.278613567352295
step: 1600, loss: 7.139894485473633
step: 1700, loss: 6.084510326385498
step: 1800, loss: 4.562003135681152
step: 1900, loss: 5.602965354919434
step: 2000, loss: 6.252871036529541
step: 2100, loss: 6.699208736419678
step: 2200, loss: 6.1359124183654785
step: 2300, loss: 4.691183090209961
step: 2400, loss: 3.8437857627868652
step: 2500, loss: 5.013026237487793
step: 2600, loss: 5.980546951293945
step: 2700, loss: 5.014110088348389
step: 2800, loss: 5.266014575958252
step: 2900, loss: 5.546093940734863
step: 3000, loss: 7.54626989364624
step: 3100, loss: 5.7562255859375
step: 3200, loss: 5.634490489959717
step: 3300, loss: 7.771360397338867
step: 3400, loss: 6.547565460205078
step: 3500, loss: 6.656982898712158
step: 3600, loss: 5.5727972984313965
step: 3700, loss: 4.828951358795166
step: 3800, loss: 4.605647563934326
step: 3900, loss: 3.568387269973755
step: 4000, loss: 5.147853374481201
step: 4100, loss: 6.27246618270874
step: 4200, loss: 7.359224319458008
step: 4300, loss: 6.357624530792236
step: 4400, loss: 6.102385520935059
step: 4500, loss: 4.682742118835449
step: 4600, loss: 4.16594934463501
step: 4700, loss: 7.880674362182617
step: 4800, loss: 6.382229328155518
step: 4900, loss: 6.899694919586182
step: 5000, loss: 6.786593437194824
step: 5100, loss: 4.0309648513793945
step: 5200, loss: 4.523194313049316
step: 5300, loss: 4.957152843475342
step: 5400, loss: 5.477055072784424
step: 5500, loss: 5.718860149383545
step: 5600, loss: 6.89410924911499
step: 5700, loss: 5.2524285316467285
step: 5800, loss: 5.448278903961182
step: 5900, loss: 3.915926456451416
step: 6000, loss: 5.571412086486816
step: 6100, loss: 3.9625911712646484
step: 6200, loss: 7.333136081695557
step: 6300, loss: 6.474406719207764
step: 6400, loss: 5.238260269165039
step: 6500, loss: 6.041682243347168
step: 6600, loss: 5.7707014083862305
step: 6700, loss: 5.717630386352539
step: 6800, loss: 4.638919353485107
step: 6900, loss: 8.046984672546387
step: 7000, loss: 4.7459397315979
step: 7100, loss: 5.437774181365967
step: 7200, loss: 5.916182994842529
step: 7300, loss: 4.818724632263184
step: 7400, loss: 6.141821384429932
step: 7500, loss: 6.013014793395996
step: 7600, loss: 5.269792556762695
step: 7700, loss: 5.8107194900512695
step: 7800, loss: 6.957659721374512
step: 7900, loss: 4.912162780761719
step: 8000, loss: 4.915109634399414
step: 8100, loss: 4.732378959655762
step: 8200, loss: 4.580258846282959
step: 8300, loss: 5.216664791107178
step: 8400, loss: 5.737919330596924
step: 8500, loss: 4.712179183959961
step: 8600, loss: 5.347327709197998
step: 8700, loss: 4.636539936065674
step: 8800, loss: 5.598361968994141
step: 8900, loss: 7.021378993988037
step: 9000, loss: 5.653539180755615
step: 9100, loss: 8.387350082397461
step: 9200, loss: 4.5909271240234375
step: 9300, loss: 5.096821308135986
step: 9400, loss: 6.20929479598999
step: 9500, loss: 3.734489917755127
step: 9600, loss: 4.307082653045654
step: 9700, loss: 5.269373893737793
step: 9800, loss: 4.0977559089660645
step: 9900, loss: 5.589142322540283
step: 10000, loss: 4.879739284515381
step: 10100, loss: 5.518553733825684
step: 10200, loss: 6.661095142364502
step: 10300, loss: 5.427572727203369
step: 10400, loss: 4.845054626464844
step: 10500, loss: 6.506349086761475
step: 10600, loss: 5.7877702713012695
step: 10700, loss: 6.577656269073486
step: 10800, loss: 6.230001449584961
step: 10900, loss: 5.665246963500977
step: 11000, loss: 6.259622097015381
step: 11100, loss: 4.345719814300537
step: 11200, loss: 6.284276962280273
step: 11300, loss: 5.701353549957275
step: 11400, loss: 7.76256799697876
step: 11500, loss: 5.54940938949585
step: 11600, loss: 5.198470115661621
step: 11700, loss: 6.749035358428955
step: 11800, loss: 5.782526016235352
step: 11900, loss: 4.665160655975342
Mean loss is 0.002936295049794164
-----------------------------------------------

RUN NUMBER 18
RUNNING FOR learning_rate: 0.001, num_units_1: 200, num_units_2: 40, k_sq: 0.5, epochs: 12000, activation_fn_1: <function sigmoid at 0x11a9b4ea0>, activation_fn_2: <function sigmoid at 0x11a9b4ea0>
-----------------------------------------------

step: 0, loss: 12.154295921325684
step: 100, loss: 6.904117107391357
step: 200, loss: 8.340609550476074
step: 300, loss: 9.828643798828125
step: 400, loss: 6.386659622192383
step: 500, loss: 7.6513776779174805
step: 600, loss: 10.00295352935791
step: 700, loss: 7.068882942199707
step: 800, loss: 7.149590015411377
step: 900, loss: 6.47559118270874
step: 1000, loss: 4.879534721374512
step: 1100, loss: 6.819445610046387
step: 1200, loss: 7.132813453674316
step: 1300, loss: 6.4260478019714355
step: 1400, loss: 5.454301834106445
step: 1500, loss: 4.980436325073242
step: 1600, loss: 6.0459794998168945
step: 1700, loss: 5.483821868896484
step: 1800, loss: 5.764063358306885
step: 1900, loss: 7.372722625732422
step: 2000, loss: 7.342252254486084
step: 2100, loss: 5.091949939727783
step: 2200, loss: 4.612346172332764
step: 2300, loss: 6.586526870727539
step: 2400, loss: 5.675310134887695
step: 2500, loss: 6.716249942779541
step: 2600, loss: 5.768568992614746
step: 2700, loss: 6.643520832061768
step: 2800, loss: 5.500685214996338
step: 2900, loss: 5.9620771408081055
step: 3000, loss: 5.449323654174805
step: 3100, loss: 5.505187511444092
step: 3200, loss: 5.643091678619385
step: 3300, loss: 5.273416042327881
step: 3400, loss: 5.589382171630859
step: 3500, loss: 4.705989360809326
step: 3600, loss: 6.013006687164307
step: 3700, loss: 5.264211177825928
step: 3800, loss: 4.644448757171631
step: 3900, loss: 4.846916675567627
step: 4000, loss: 6.514062404632568
step: 4100, loss: 7.5349860191345215
step: 4200, loss: 4.738370895385742
step: 4300, loss: 5.293247699737549
step: 4400, loss: 6.316738605499268
step: 4500, loss: 4.3917388916015625
step: 4600, loss: 4.8295183181762695
step: 4700, loss: 5.923289775848389
step: 4800, loss: 7.939561367034912
step: 4900, loss: 5.212243556976318
step: 5000, loss: 6.429353713989258
step: 5100, loss: 6.540979385375977
step: 5200, loss: 4.296541213989258
step: 5300, loss: 4.845996856689453
step: 5400, loss: 3.6191349029541016
step: 5500, loss: 6.6207685470581055
step: 5600, loss: 6.204402923583984
step: 5700, loss: 5.612027645111084
step: 5800, loss: 4.180119037628174
step: 5900, loss: 4.434724807739258
step: 6000, loss: 4.486583232879639
step: 6100, loss: 8.495463371276855
step: 6200, loss: 5.807697772979736
step: 6300, loss: 6.16121244430542
step: 6400, loss: 4.4699015617370605
step: 6500, loss: 6.4106903076171875
step: 6600, loss: 3.8493621349334717
step: 6700, loss: 5.453344821929932
step: 6800, loss: 5.276115894317627
step: 6900, loss: 6.820178985595703
step: 7000, loss: 5.1494221687316895
step: 7100, loss: 6.236093044281006
step: 7200, loss: 4.191289901733398
step: 7300, loss: 6.10067892074585
step: 7400, loss: 7.5566229820251465
step: 7500, loss: 5.777782440185547
step: 7600, loss: 5.828631401062012
step: 7700, loss: 6.667437553405762
step: 7800, loss: 4.7605366706848145
step: 7900, loss: 6.1292924880981445
step: 8000, loss: 5.577720642089844
step: 8100, loss: 6.6373467445373535
step: 8200, loss: 5.041659832000732
step: 8300, loss: 6.261962413787842
step: 8400, loss: 5.7075347900390625
step: 8500, loss: 5.938284873962402
step: 8600, loss: 5.888147354125977
step: 8700, loss: 5.583204746246338
step: 8800, loss: 6.3639726638793945
step: 8900, loss: 4.648780822753906
step: 9000, loss: 5.9012908935546875
step: 9100, loss: 5.328988075256348
step: 9200, loss: 4.453227996826172
step: 9300, loss: 4.757234573364258
step: 9400, loss: 5.229104042053223
step: 9500, loss: 5.9410719871521
step: 9600, loss: 6.559424877166748
step: 9700, loss: 5.113255023956299
step: 9800, loss: 5.933483123779297
step: 9900, loss: 4.388027667999268
step: 10000, loss: 6.517030239105225
step: 10100, loss: 6.907026767730713
step: 10200, loss: 5.369773864746094
step: 10300, loss: 6.329838275909424
step: 10400, loss: 6.699525833129883
step: 10500, loss: 5.101705074310303
step: 10600, loss: 3.961030960083008
step: 10700, loss: 5.410194396972656
step: 10800, loss: 6.573551654815674
step: 10900, loss: 6.037729740142822
step: 11000, loss: 6.452301979064941
step: 11100, loss: 5.495761871337891
step: 11200, loss: 7.001598358154297
step: 11300, loss: 6.155517101287842
step: 11400, loss: 5.31880521774292
step: 11500, loss: 4.42988920211792
step: 11600, loss: 6.820358753204346
step: 11700, loss: 5.947786331176758
step: 11800, loss: 6.037256717681885
step: 11900, loss: 5.7913498878479
Mean loss is 0.002940976027473982
-----------------------------------------------

