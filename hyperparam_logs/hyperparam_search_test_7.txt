RUN NUMBER 1
Numpy random seed 1
Learning rate 0.0001
Decay rate: 0.9999999999
N_units_1: 100
N_units_2: 20
Activation_fn_1: <function sigmoid at 0x11b6a5ea0>, activation_fn_2: <function sigmoid at 0x11b6a5ea0>
k_squared: 0.5
Number of epochs: 100000
-----------------------------------------------

step: 0, loss: 7.809102535247803
step: 1000, loss: 10.202803611755371
step: 2000, loss: 7.516639232635498
step: 3000, loss: 8.265339851379395
step: 4000, loss: 7.109211444854736
step: 5000, loss: 6.3934855461120605
step: 6000, loss: 6.749963760375977
step: 7000, loss: 7.003159999847412
step: 8000, loss: 7.5246663093566895
step: 9000, loss: 6.298841953277588
step: 10000, loss: 4.7304229736328125
step: 11000, loss: 7.513708114624023
step: 12000, loss: 6.27223014831543
step: 13000, loss: 6.823930740356445
step: 14000, loss: 5.993576526641846
step: 15000, loss: 6.793493270874023
step: 16000, loss: 6.7316060066223145
step: 17000, loss: 7.866238594055176
step: 18000, loss: 6.164898872375488
step: 19000, loss: 4.858319282531738
step: 20000, loss: 8.982345581054688
step: 21000, loss: 5.647677421569824
step: 22000, loss: 6.3173651695251465
step: 23000, loss: 5.913166046142578
step: 24000, loss: 6.075764179229736
step: 25000, loss: 6.36132287979126
step: 26000, loss: 6.355039596557617
step: 27000, loss: 5.07777214050293
step: 28000, loss: 5.14119815826416
step: 29000, loss: 6.068986415863037
step: 30000, loss: 7.493570804595947
step: 31000, loss: 5.169883728027344
step: 32000, loss: 5.518660068511963
step: 33000, loss: 5.2385735511779785
step: 34000, loss: 5.364668846130371
step: 35000, loss: 5.7581400871276855
step: 36000, loss: 5.723029613494873
step: 37000, loss: 5.219577789306641
step: 38000, loss: 5.436389923095703
step: 39000, loss: 5.018767833709717
step: 40000, loss: 5.756753921508789
step: 41000, loss: 4.55030632019043
step: 42000, loss: 4.231731414794922
step: 43000, loss: 4.231985092163086
step: 44000, loss: 5.983924865722656
step: 45000, loss: 5.94381856918335
step: 46000, loss: 6.548001289367676
step: 47000, loss: 5.829077243804932
step: 48000, loss: 4.782776355743408
step: 49000, loss: 5.571122169494629
step: 50000, loss: 6.510684013366699
step: 51000, loss: 5.895968437194824
step: 52000, loss: 5.876157760620117
step: 53000, loss: 4.916952133178711
step: 54000, loss: 6.060691833496094
step: 55000, loss: 6.225491046905518
step: 56000, loss: 5.588901996612549
step: 57000, loss: 4.465202331542969
step: 58000, loss: 4.545884609222412
step: 59000, loss: 4.520946979522705
step: 60000, loss: 5.946043491363525
step: 61000, loss: 5.821634292602539
step: 62000, loss: 5.8350396156311035
step: 63000, loss: 4.648652076721191
step: 64000, loss: 5.130999565124512
step: 65000, loss: 7.690340042114258
step: 66000, loss: 5.241418838500977
step: 67000, loss: 5.638394832611084
step: 68000, loss: 6.140053749084473
step: 69000, loss: 6.366137981414795
step: 70000, loss: 4.3193817138671875
step: 71000, loss: 3.191227436065674
step: 72000, loss: 5.588531970977783
step: 73000, loss: 6.277584552764893
step: 74000, loss: 5.237294673919678
step: 75000, loss: 5.327337265014648
step: 76000, loss: 6.211336135864258
step: 77000, loss: 5.227288722991943
step: 78000, loss: 6.699715614318848
step: 79000, loss: 4.805728912353516
step: 80000, loss: 7.343900680541992
step: 81000, loss: 3.8581504821777344
step: 82000, loss: 7.224003791809082
step: 83000, loss: 6.985283851623535
step: 84000, loss: 4.786197185516357
step: 85000, loss: 4.281160354614258
step: 86000, loss: 6.198526859283447
step: 87000, loss: 4.960946083068848
step: 88000, loss: 5.075878143310547
step: 89000, loss: 5.367995738983154
step: 90000, loss: 4.91168212890625
step: 91000, loss: 5.731521129608154
step: 92000, loss: 4.604501724243164
step: 93000, loss: 5.517582893371582
step: 94000, loss: 3.5819668769836426
step: 95000, loss: 4.755157470703125
step: 96000, loss: 4.8817524909973145
step: 97000, loss: 6.938288688659668
step: 98000, loss: 4.094343662261963
step: 99000, loss: 5.896644592285156
Mean loss is 0.0029554824173444534
-----------------------------------------------

