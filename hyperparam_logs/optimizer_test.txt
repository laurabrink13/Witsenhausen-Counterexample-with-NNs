Summary: 
Adam and RMSProp slightly outperform vanilla GD, but not by much.

RUN NUMBER 1
Numpy random seed 1
Initial learning rate 0.0001
GD Optimizier <class 'tensorflow.python.training.adam.AdamOptimizer'>
Decay rate: 0.999 is UNUSED
N_units_1: 150
N_units_2: 30
Activation_fn_1: <function sigmoid at 0x117a48ea0>, activation_fn_2: <function sigmoid at 0x117a48ea0>
k_squared: 0.5
Number of epochs: 100000
-----------------------------------------------

step: 0, loss: 7.017581939697266, lr: 0.0001
step: 1000, loss: 7.524717807769775, lr: 0.0001
step: 2000, loss: 5.19110631942749, lr: 0.0001
step: 3000, loss: 5.985276222229004, lr: 0.0001
step: 4000, loss: 5.054803371429443, lr: 0.0001
step: 5000, loss: 4.56358003616333, lr: 0.0001
step: 6000, loss: 5.172049045562744, lr: 0.0001
step: 7000, loss: 5.439204216003418, lr: 0.0001
step: 8000, loss: 5.987191677093506, lr: 0.0001
step: 9000, loss: 5.1139302253723145, lr: 0.0001
step: 10000, loss: 3.665073871612549, lr: 0.0001
step: 11000, loss: 6.320647716522217, lr: 0.0001
step: 12000, loss: 5.289621353149414, lr: 0.0001
step: 13000, loss: 5.829526901245117, lr: 0.0001
step: 14000, loss: 5.110173225402832, lr: 0.0001
step: 15000, loss: 5.840584754943848, lr: 0.0001
step: 16000, loss: 6.026463985443115, lr: 0.0001
step: 17000, loss: 7.074948310852051, lr: 0.0001
step: 18000, loss: 5.496356964111328, lr: 0.0001
step: 19000, loss: 4.31352424621582, lr: 0.0001
step: 20000, loss: 8.303878784179688, lr: 0.0001
step: 21000, loss: 5.152365207672119, lr: 0.0001
step: 22000, loss: 5.738340377807617, lr: 0.0001
step: 23000, loss: 5.401036739349365, lr: 0.0001
step: 24000, loss: 5.564008712768555, lr: 0.0001
step: 25000, loss: 5.828829765319824, lr: 0.0001
step: 26000, loss: 5.949952125549316, lr: 0.0001
step: 27000, loss: 4.667876720428467, lr: 0.0001
step: 28000, loss: 4.665130138397217, lr: 0.0001
step: 29000, loss: 5.668708324432373, lr: 0.0001
step: 30000, loss: 7.082364082336426, lr: 0.0001
step: 31000, loss: 4.8161234855651855, lr: 0.0001
step: 32000, loss: 5.182584762573242, lr: 0.0001
step: 33000, loss: 4.954291343688965, lr: 0.0001
step: 34000, loss: 5.030426979064941, lr: 0.0001
step: 35000, loss: 5.490139961242676, lr: 0.0001
step: 36000, loss: 5.397725582122803, lr: 0.0001
step: 37000, loss: 4.922841548919678, lr: 0.0001
step: 38000, loss: 5.1276068687438965, lr: 0.0001
step: 39000, loss: 4.773947238922119, lr: 0.0001
step: 40000, loss: 5.458537578582764, lr: 0.0001
step: 41000, loss: 4.248724937438965, lr: 0.0001
step: 42000, loss: 3.9288594722747803, lr: 0.0001
step: 43000, loss: 3.958467483520508, lr: 0.0001
step: 44000, loss: 5.754262924194336, lr: 0.0001
step: 45000, loss: 5.6394267082214355, lr: 0.0001
step: 46000, loss: 6.280341148376465, lr: 0.0001
step: 47000, loss: 5.614407539367676, lr: 0.0001
step: 48000, loss: 4.5428290367126465, lr: 0.0001
step: 49000, loss: 5.3668646812438965, lr: 0.0001
step: 50000, loss: 6.267219066619873, lr: 0.0001
step: 51000, loss: 5.717778205871582, lr: 0.0001
step: 52000, loss: 5.659322261810303, lr: 0.0001
step: 53000, loss: 4.658367156982422, lr: 0.0001
step: 54000, loss: 5.856475830078125, lr: 0.0001
step: 55000, loss: 6.02750825881958, lr: 0.0001
step: 56000, loss: 5.426377296447754, lr: 0.0001
step: 57000, loss: 4.2941179275512695, lr: 0.0001
step: 58000, loss: 4.363727569580078, lr: 0.0001
step: 59000, loss: 4.324155807495117, lr: 0.0001
step: 60000, loss: 5.738831520080566, lr: 0.0001
step: 61000, loss: 5.656252384185791, lr: 0.0001
step: 62000, loss: 5.623647212982178, lr: 0.0001
step: 63000, loss: 4.459906578063965, lr: 0.0001
step: 64000, loss: 4.944113254547119, lr: 0.0001
step: 65000, loss: 7.56481409072876, lr: 0.0001
step: 66000, loss: 5.10822868347168, lr: 0.0001
step: 67000, loss: 5.475729942321777, lr: 0.0001
step: 68000, loss: 5.966676712036133, lr: 0.0001
step: 69000, loss: 6.250153064727783, lr: 0.0001
step: 70000, loss: 4.190885066986084, lr: 0.0001
step: 71000, loss: 3.0600523948669434, lr: 0.0001
step: 72000, loss: 5.4597249031066895, lr: 0.0001
step: 73000, loss: 6.133424282073975, lr: 0.0001
step: 74000, loss: 5.101569175720215, lr: 0.0001
step: 75000, loss: 5.173215866088867, lr: 0.0001
step: 76000, loss: 6.095027923583984, lr: 0.0001
step: 77000, loss: 5.124847412109375, lr: 0.0001
step: 78000, loss: 6.596888065338135, lr: 0.0001
step: 79000, loss: 4.626989364624023, lr: 0.0001
step: 80000, loss: 7.187687397003174, lr: 0.0001
step: 81000, loss: 3.7064805030822754, lr: 0.0001
step: 82000, loss: 7.136971473693848, lr: 0.0001
step: 83000, loss: 6.8298258781433105, lr: 0.0001
step: 84000, loss: 4.668403148651123, lr: 0.0001
step: 85000, loss: 4.1552205085754395, lr: 0.0001
step: 86000, loss: 6.088309288024902, lr: 0.0001
step: 87000, loss: 4.8068623542785645, lr: 0.0001
step: 88000, loss: 4.938905239105225, lr: 0.0001
step: 89000, loss: 5.216604709625244, lr: 0.0001
step: 90000, loss: 4.756680488586426, lr: 0.0001
step: 91000, loss: 5.641963005065918, lr: 0.0001
step: 92000, loss: 4.4996562004089355, lr: 0.0001
step: 93000, loss: 5.39877462387085, lr: 0.0001
step: 94000, loss: 3.4635655879974365, lr: 0.0001
step: 95000, loss: 4.632079601287842, lr: 0.0001
step: 96000, loss: 4.802166938781738, lr: 0.0001
step: 97000, loss: 6.850022792816162, lr: 0.0001
step: 98000, loss: 3.9534554481506348, lr: 0.0001
step: 99000, loss: 5.808320999145508, lr: 0.0001
Mean loss is 0.002901128633824228
-----------------------------------------------

RUN NUMBER 2
Numpy random seed 2
Initial learning rate 0.0001
GD Optimizier <class 'tensorflow.python.training.rmsprop.RMSPropOptimizer'>
Decay rate: 0.999 is UNUSED
N_units_1: 150
N_units_2: 30
Activation_fn_1: <function sigmoid at 0x117a48ea0>, activation_fn_2: <function sigmoid at 0x117a48ea0>
k_squared: 0.5
Number of epochs: 100000
-----------------------------------------------

step: 0, loss: 9.78895378112793, lr: 0.0001
step: 1000, loss: 5.724827766418457, lr: 0.0001
step: 2000, loss: 4.1638617515563965, lr: 0.0001
step: 3000, loss: 8.363998413085938, lr: 0.0001
step: 4000, loss: 6.342334270477295, lr: 0.0001
step: 5000, loss: 6.094432830810547, lr: 0.0001
step: 6000, loss: 5.0016560554504395, lr: 0.0001
step: 7000, loss: 6.697092533111572, lr: 0.0001
step: 8000, loss: 5.9368720054626465, lr: 0.0001
step: 9000, loss: 5.163447380065918, lr: 0.0001
step: 10000, loss: 4.619987964630127, lr: 0.0001
step: 11000, loss: 4.700000286102295, lr: 0.0001
step: 12000, loss: 5.750802040100098, lr: 0.0001
step: 13000, loss: 5.447001934051514, lr: 0.0001
step: 14000, loss: 5.421069145202637, lr: 0.0001
step: 15000, loss: 5.794748306274414, lr: 0.0001
step: 16000, loss: 4.905473232269287, lr: 0.0001
step: 17000, loss: 6.12710428237915, lr: 0.0001
step: 18000, loss: 5.409998893737793, lr: 0.0001
step: 19000, loss: 6.491565227508545, lr: 0.0001
step: 20000, loss: 4.718557357788086, lr: 0.0001
step: 21000, loss: 5.9205756187438965, lr: 0.0001
step: 22000, loss: 6.608048439025879, lr: 0.0001
step: 23000, loss: 3.54460072517395, lr: 0.0001
step: 24000, loss: 5.287288188934326, lr: 0.0001
step: 25000, loss: 6.714215278625488, lr: 0.0001
step: 26000, loss: 4.575896739959717, lr: 0.0001
step: 27000, loss: 4.733623504638672, lr: 0.0001
step: 28000, loss: 5.119731426239014, lr: 0.0001
step: 29000, loss: 4.302871227264404, lr: 0.0001
step: 30000, loss: 5.785557270050049, lr: 0.0001
step: 31000, loss: 5.425285339355469, lr: 0.0001
step: 32000, loss: 4.885580539703369, lr: 0.0001
step: 33000, loss: 4.962800979614258, lr: 0.0001
step: 34000, loss: 4.477659702301025, lr: 0.0001
step: 35000, loss: 4.23643159866333, lr: 0.0001
step: 36000, loss: 6.762118816375732, lr: 0.0001
step: 37000, loss: 5.927847385406494, lr: 0.0001
step: 38000, loss: 4.928439140319824, lr: 0.0001
step: 39000, loss: 5.3418989181518555, lr: 0.0001
step: 40000, loss: 7.114381790161133, lr: 0.0001
step: 41000, loss: 3.9449901580810547, lr: 0.0001
step: 42000, loss: 5.470458030700684, lr: 0.0001
step: 43000, loss: 3.8206090927124023, lr: 0.0001
step: 44000, loss: 4.813720226287842, lr: 0.0001
step: 45000, loss: 5.429044246673584, lr: 0.0001
step: 46000, loss: 3.426819324493408, lr: 0.0001
step: 47000, loss: 4.870695114135742, lr: 0.0001
step: 48000, loss: 5.453251361846924, lr: 0.0001
step: 49000, loss: 6.885589599609375, lr: 0.0001
step: 50000, loss: 6.031984329223633, lr: 0.0001
step: 51000, loss: 6.328801155090332, lr: 0.0001
step: 52000, loss: 4.0372514724731445, lr: 0.0001
step: 53000, loss: 5.088217735290527, lr: 0.0001
step: 54000, loss: 7.000926494598389, lr: 0.0001
step: 55000, loss: 5.661972999572754, lr: 0.0001
step: 56000, loss: 4.514402389526367, lr: 0.0001
step: 57000, loss: 7.4177632331848145, lr: 0.0001
step: 58000, loss: 6.586965560913086, lr: 0.0001
step: 59000, loss: 5.2862091064453125, lr: 0.0001
step: 60000, loss: 6.250945568084717, lr: 0.0001
step: 61000, loss: 4.816113471984863, lr: 0.0001
step: 62000, loss: 7.271581172943115, lr: 0.0001
step: 63000, loss: 5.463040351867676, lr: 0.0001
step: 64000, loss: 5.264557361602783, lr: 0.0001
step: 65000, loss: 5.396295547485352, lr: 0.0001
step: 66000, loss: 6.165008544921875, lr: 0.0001
step: 67000, loss: 4.880420207977295, lr: 0.0001
step: 68000, loss: 6.155576229095459, lr: 0.0001
step: 69000, loss: 4.456510543823242, lr: 0.0001
step: 70000, loss: 6.359004497528076, lr: 0.0001
step: 71000, loss: 5.372729778289795, lr: 0.0001
step: 72000, loss: 5.1860032081604, lr: 0.0001
step: 73000, loss: 4.836111545562744, lr: 0.0001
step: 74000, loss: 6.769779205322266, lr: 0.0001
step: 75000, loss: 5.246021747589111, lr: 0.0001
step: 76000, loss: 5.651224136352539, lr: 0.0001
step: 77000, loss: 4.92540168762207, lr: 0.0001
step: 78000, loss: 6.997348308563232, lr: 0.0001
step: 79000, loss: 5.709403038024902, lr: 0.0001
step: 80000, loss: 4.954411506652832, lr: 0.0001
step: 81000, loss: 6.187145233154297, lr: 0.0001
step: 82000, loss: 5.473468780517578, lr: 0.0001
step: 83000, loss: 4.484253406524658, lr: 0.0001
step: 84000, loss: 5.577983379364014, lr: 0.0001
step: 85000, loss: 4.593873023986816, lr: 0.0001
step: 86000, loss: 5.551985740661621, lr: 0.0001
step: 87000, loss: 6.034673690795898, lr: 0.0001
step: 88000, loss: 5.589387893676758, lr: 0.0001
step: 89000, loss: 5.802093982696533, lr: 0.0001
step: 90000, loss: 4.740118503570557, lr: 0.0001
step: 91000, loss: 4.418169021606445, lr: 0.0001
step: 92000, loss: 5.828734397888184, lr: 0.0001
step: 93000, loss: 5.272364139556885, lr: 0.0001
step: 94000, loss: 4.693086624145508, lr: 0.0001
step: 95000, loss: 3.665104389190674, lr: 0.0001
step: 96000, loss: 5.320506572723389, lr: 0.0001
step: 97000, loss: 4.968636512756348, lr: 0.0001
step: 98000, loss: 6.338042736053467, lr: 0.0001
step: 99000, loss: 5.810328960418701, lr: 0.0001
Mean loss is 0.0029040242542450263
-----------------------------------------------

RUN NUMBER 3
Numpy random seed 3
Initial learning rate 0.0001
GD Optimizier <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>
Decay rate: 0.999 is UNUSED
N_units_1: 150
N_units_2: 30
Activation_fn_1: <function sigmoid at 0x117a48ea0>, activation_fn_2: <function sigmoid at 0x117a48ea0>
k_squared: 0.5
Number of epochs: 100000
-----------------------------------------------

step: 0, loss: 10.270317077636719, lr: 0.0001
step: 1000, loss: 10.226303100585938, lr: 0.0001
step: 2000, loss: 7.882264614105225, lr: 0.0001
step: 3000, loss: 6.865666389465332, lr: 0.0001
step: 4000, loss: 7.100042819976807, lr: 0.0001
step: 5000, loss: 6.333362579345703, lr: 0.0001
step: 6000, loss: 5.191410064697266, lr: 0.0001
step: 7000, loss: 8.739831924438477, lr: 0.0001
step: 8000, loss: 5.722460746765137, lr: 0.0001
step: 9000, loss: 6.265629768371582, lr: 0.0001
step: 10000, loss: 5.674626350402832, lr: 0.0001
step: 11000, loss: 5.92656135559082, lr: 0.0001
step: 12000, loss: 5.2332377433776855, lr: 0.0001
step: 13000, loss: 5.148509979248047, lr: 0.0001
step: 14000, loss: 5.248893737792969, lr: 0.0001
step: 15000, loss: 5.774387836456299, lr: 0.0001
step: 16000, loss: 6.916177749633789, lr: 0.0001
step: 17000, loss: 4.705593585968018, lr: 0.0001
step: 18000, loss: 5.312978267669678, lr: 0.0001
step: 19000, loss: 4.794760227203369, lr: 0.0001
step: 20000, loss: 5.835719585418701, lr: 0.0001
step: 21000, loss: 5.213621616363525, lr: 0.0001
step: 22000, loss: 7.240894317626953, lr: 0.0001
step: 23000, loss: 6.581523418426514, lr: 0.0001
step: 24000, loss: 6.375192642211914, lr: 0.0001
step: 25000, loss: 6.2165069580078125, lr: 0.0001
step: 26000, loss: 4.353827476501465, lr: 0.0001
step: 27000, loss: 4.393362998962402, lr: 0.0001
step: 28000, loss: 6.528046607971191, lr: 0.0001
step: 29000, loss: 5.6967244148254395, lr: 0.0001
step: 30000, loss: 6.021974563598633, lr: 0.0001
step: 31000, loss: 5.206309795379639, lr: 0.0001
step: 32000, loss: 6.024878025054932, lr: 0.0001
step: 33000, loss: 5.0264811515808105, lr: 0.0001
step: 34000, loss: 6.084731101989746, lr: 0.0001
step: 35000, loss: 5.733831405639648, lr: 0.0001
step: 36000, loss: 4.691682815551758, lr: 0.0001
step: 37000, loss: 6.006346225738525, lr: 0.0001
step: 38000, loss: 6.381694316864014, lr: 0.0001
step: 39000, loss: 7.088510990142822, lr: 0.0001
step: 40000, loss: 6.0472822189331055, lr: 0.0001
step: 41000, loss: 6.438443183898926, lr: 0.0001
step: 42000, loss: 5.545517921447754, lr: 0.0001
step: 43000, loss: 5.865830898284912, lr: 0.0001
step: 44000, loss: 4.794826030731201, lr: 0.0001
step: 45000, loss: 5.474850654602051, lr: 0.0001
step: 46000, loss: 6.782410621643066, lr: 0.0001
step: 47000, loss: 5.555841445922852, lr: 0.0001
step: 48000, loss: 4.608452796936035, lr: 0.0001
step: 49000, loss: 6.161395072937012, lr: 0.0001
step: 50000, loss: 5.676931858062744, lr: 0.0001
step: 51000, loss: 6.4373931884765625, lr: 0.0001
step: 52000, loss: 4.4772162437438965, lr: 0.0001
step: 53000, loss: 4.638336658477783, lr: 0.0001
step: 54000, loss: 4.390719890594482, lr: 0.0001
step: 55000, loss: 4.860761642456055, lr: 0.0001
step: 56000, loss: 6.483584880828857, lr: 0.0001
step: 57000, loss: 6.40089750289917, lr: 0.0001
step: 58000, loss: 4.655650615692139, lr: 0.0001
step: 59000, loss: 5.548314094543457, lr: 0.0001
step: 60000, loss: 6.930946350097656, lr: 0.0001
step: 61000, loss: 5.555689811706543, lr: 0.0001
step: 62000, loss: 6.0878190994262695, lr: 0.0001
step: 63000, loss: 5.956003665924072, lr: 0.0001
step: 64000, loss: 5.56504487991333, lr: 0.0001
step: 65000, loss: 6.791018962860107, lr: 0.0001
step: 66000, loss: 5.273842811584473, lr: 0.0001
step: 67000, loss: 4.5714497566223145, lr: 0.0001
step: 68000, loss: 4.935874938964844, lr: 0.0001
step: 69000, loss: 5.407485008239746, lr: 0.0001
step: 70000, loss: 4.89123010635376, lr: 0.0001
step: 71000, loss: 6.541970252990723, lr: 0.0001
step: 72000, loss: 6.943835735321045, lr: 0.0001
step: 73000, loss: 5.191390514373779, lr: 0.0001
step: 74000, loss: 4.474761486053467, lr: 0.0001
step: 75000, loss: 6.580784797668457, lr: 0.0001
step: 76000, loss: 5.838748931884766, lr: 0.0001
step: 77000, loss: 4.2069172859191895, lr: 0.0001
step: 78000, loss: 5.469413757324219, lr: 0.0001
step: 79000, loss: 4.534345626831055, lr: 0.0001
step: 80000, loss: 4.654932498931885, lr: 0.0001
step: 81000, loss: 5.388884544372559, lr: 0.0001
step: 82000, loss: 4.8627028465271, lr: 0.0001
step: 83000, loss: 5.380143165588379, lr: 0.0001
step: 84000, loss: 4.800379753112793, lr: 0.0001
step: 85000, loss: 6.106119632720947, lr: 0.0001
step: 86000, loss: 4.935248374938965, lr: 0.0001
step: 87000, loss: 5.0160651206970215, lr: 0.0001
step: 88000, loss: 5.495522499084473, lr: 0.0001
step: 89000, loss: 5.341039180755615, lr: 0.0001
step: 90000, loss: 5.449893474578857, lr: 0.0001
step: 91000, loss: 5.337602615356445, lr: 0.0001
step: 92000, loss: 4.560469150543213, lr: 0.0001
step: 93000, loss: 4.256927013397217, lr: 0.0001
step: 94000, loss: 5.752938747406006, lr: 0.0001
step: 95000, loss: 7.454318046569824, lr: 0.0001
step: 96000, loss: 6.0797438621521, lr: 0.0001
step: 97000, loss: 3.6777799129486084, lr: 0.0001
step: 98000, loss: 4.828107833862305, lr: 0.0001
step: 99000, loss: 6.2091383934021, lr: 0.0001
Mean loss is 0.0029453493392970613
-----------------------------------------------

